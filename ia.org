#+TITLE: Apuntes IA
#+AUTHOR: Eduardo Alcaraz
#+LANGUAGE: es
#+LaTeX_HEADER: \usepackage[spanish]{inputenc}
#+SETUPFILE: /home/likcos/Materias/IA/theme-readtheorg-local.setup
#+EXPORT_FILE_NAME: index.html
#+OPTIONS: num:nil
#+HTML_HEAD: <style> #content{max-width:1800px;}</style>
#+HTML_HEAD: <style>pre.src {background-color: #303030; color: #e5e5e5;}</style>



*  Introducción a la Inteligencia Artificial.
 La inteligencia artificial (IA) es un área multidisciplinaria que, a
  través de ciencias como las ciencias de la computación, la lógica y la
  filosofía, estudia la creación y diseño de entidades capaces de
  resolver cuestiones por sí mismas utilizando como paradigma la
  inteligencia humana.


  General y amplio como eso, reúne a amplios campos, los cuales tienen
  en común la creación de máquinas capaces de pensar. En ciencias de la
  computación se denomina inteligencia artificial a la capacidad de
  razonar de un agente no vivo. John McCarthy acuñó la expresión
  inteligencia artificial en 1956, y la definió así: Es la ciencia e
  ingenio de hacer máquinas inteligentes, especialmente programas de
  cómputo inteligentes.

  - Búsqueda del estado requerido en el conjunto de los estados
	producidos por las acciones posibles.
  - Algoritmos genéticos (análogo al proceso de evolución de las cadenas
	de ADN).
  - Redes neuronales artificiales (análogo al funcionamiento físico del
	cerebro de animales y humanos).  
  - Razonamiento mediante una lógica formal análogo al pensamiento
	abstracto humano.  


  También existen distintos tipos de percepciones y acciones, que pueden
  ser obtenidas y producidas, respectivamente, por sensores físicos y
  sensores mecánicos en máquinas, pulsos eléctricos u ópticos en
  computadoras, tanto como por entradas y salidas de bits de un software
  y su entorno software.
  Varios ejemplos se encuentran en el área de control de sistemas,
  planificación automática, la habilidad de responder a diagnósticos y a
  consultas de los consumidores, reconocimiento de escritura,
  reconocimiento del habla y reconocimiento de patrones. Los sistemas de
  IA actualmente son parte de la rutina en campos como economía,
  medicina, ingeniería y la milicia, y se ha usado en gran variedad de
  aplicaciones de software, juegos de estrategia, como ajedrez de
  computador, y otros videojuegos.

** Categorías de la Inteligencia Artificial 

   - *Sistemas que piensan como humanos*. Estos sistemas tratan de emular
     el pensamiento humano; por ejemplo las redes neuronales
     artificiales. La automatización de actividades que vinculamos con
     procesos de pensamiento humano, actividades como la Toma de
     decisiones, Resolución de problemas y aprendizaje. cite:Russell
   - *Sistemas que actúan como humanos*: Estos sistemas tratan de actuar
     como humanos; es decir, imitan el comportamiento humano; por
     ejemplo la robótica. El estudio de cómo lograr que los
     computadores realicen tareas que, por el momento, los humanos hacen mejor. cite:Russell
   - *Sistemas que piensan racionalmente*.- Es decir, con lógica (idealmente), tratan de
     imitar o emular el pensamiento lógico racional del ser humano; por ejemplo los sistemas
     expertos. El estudio de los cálculos que hacen posible percibir, razonar y actuar. cite:Russell
   - *Sistemas que actúan racionalmente (idealmente)*.– Tratan de emular de forma
     racional el comportamiento humano; por ejemplo los agentes inteligentes.Está relacionado con
     conductas inteligentes en artefactos. cite:Russell
 
**  Inteligencia artificial convencional

  Se conoce también como IA simbólico-deductiva. Está basada en el análisis formal y
  estadístico del comportamiento humano ante diferentes problemas:
 
   - Razonamiento basado en casos: Ayuda a tomar decisiones mientras se
     resuelven ciertos problemas concretos y, aparte de que son muy
     importantes, requieren de un buen funcionamiento.
   - Sistemas expertos: Infieren una solución a través del conocimiento
     previo del contexto en que se aplica y ocupa de ciertas reglas o
     relaciones.
   - Redes bayesianas: Propone soluciones mediante inferencia probabilística.
   - Inteligencia artificial basada en comportamientos: Esta
     inteligencia contiene autonomía y puede auto-regularse y
     controlarse para mejorar.
   - Smart process management: Facilita la toma de decisiones
     complejas, proponiendo una solución a un determinado problema al
     igual que lo haría un especialista en la dicha actividad.


**  Historia de la Inteligencia Artificial.
**  Las habilidades cognoscitivas según la psicología. Teorías de la inteligencia (conductismo, Gardner, etc.).

Las habilidades cognoscitivas son capacidades mentales que nos
permiten procesar toda la información que nos llega del
entorno. Incluyen procesos como la percepción, la memoria, el
aprendizaje, la solución de problemas, el razonamiento y el
pensamiento crítico. La psicología ha estudiado extensamente estas
habilidades, y diferentes teorías han surgido para explicar cómo se
desarrollan y funcionan, especialmente en relación con la
inteligencia.

***  Teorías de la Inteligencia

 1. *Conductismo* El conductismo, representado por figuras como
    John B. Watson y B.F. Skinner, se centra en el estudio de
    comportamientos observables y medibles, dejando de lado los procesos
    mentales internos. Desde esta perspectiva, la inteligencia se ve como
    una serie de respuestas aprendidas ante estímulos específicos. Los
    conductistas creen que cualquier diferencia en la inteligencia entre
    individuos se debe a diferencias en sus experiencias de
    aprendizaje. Aunque esta teoría ha sido crítica por su falta de
    atención a los procesos cognitivos internos, ha sido influyente en el
    desarrollo de técnicas de aprendizaje y modificación del
    comportamiento.

 2. *Teoría de las Inteligencias Múltiples (Howard Gardner)*
     Contrastando con el enfoque unitario de la inteligencia, Howard
     Gardner propuso en 1983 la Teoría de las Inteligencias
     Múltiples. Gardner argumentó que la inteligencia no es un dominio
     único y general, sino un conjunto de capacidades cognitivas distintas
     e independientes. Originalmente identificó siete inteligencias
     (lingüístico-verbal, lógico-matemática, espacial, musical,
     corporal-cinestésica, interpersonal e intrapersonal), a las cuales
     más tarde añadió la inteligencia naturalista y posiblemente
     otras. Esta teoría ha tenido un impacto significativo en la
     educación, promoviendo un enfoque más personalizado en la enseñanza.

 3. *Teoría Triárquica de la Inteligencia (Robert Sternberg)*
     Robert Sternberg propuso la Teoría Triárquica de la Inteligencia, que
     divide la inteligencia en tres aspectos: analítico, creativo y
     práctico. La inteligencia analítica se refiere a la capacidad de
     analizar, evaluar, juzgar, comparar y contrastar. La inteligencia
     creativa implica la capacidad de crear, diseñar, inventar, originar y
     imaginar. La inteligencia práctica se relaciona con la capacidad de
     usar, aplicar, implementar y poner en práctica. Sternberg sugiere que
     una inteligencia equilibrada implica la capacidad de adaptarse a,
     moldear y seleccionar entornos para satisfacer tanto las necesidades
     personales como las de la sociedad.

 4. *Teoría del Procesamiento de la Información*
     Esta teoría se enfoca en cómo las personas procesan la información
     que reciben. Implica la atención, percepción, memoria y pensamiento,
     y cómo estas operaciones mentales influyen en nuestra capacidad para
     resolver problemas y tomar decisiones. Desde esta perspectiva, la
     inteligencia se ve como un conjunto de procesos mentales que permiten
     a la persona comprender y manejar el mundo que la rodea.

 5. *Inteligencia Emocional (Daniel Goleman)*
    Daniel Goleman popularizó el concepto de inteligencia emocional en
    los años 90, definiéndola como la capacidad para reconocer, entender
    y manejar nuestras emociones y las de los demás. La inteligencia
    emocional incluye habilidades como la autoconciencia, la
    autoregulación, la motivación, la empatía y las habilidades
    sociales. Esta teoría amplió el concepto de inteligencia más allá de
    las capacidades cognitivas tradicionales, incluyendo aspectos
    emocionales y sociales.

 *Conclusión*: Las teorías de la inteligencia en la psicología
 reflejan la complejidad y diversidad de la mente humana. Desde el
 conductismo, que enfatiza el aprendizaje observable, hasta las
 teorías de inteligencias múltiples y emocional, que reconocen una
 amplia gama de capacidades cognitivas y emocionales, estas teorías
 nos ofrecen diferentes puntos de vista a través de los cuales podemos entender
 la inteligencia humana. Cada teoría aporta su visión única,
 sugiriendo que la inteligencia es un fenómeno multifacético que no
 puede ser completamente comprendido a través de un solo enfoque.

**  El proceso de razonamiento según la lógica (Axiomas, Teoremas, demostración).
El proceso de razonamiento según la lógica involucra varios conceptos
fundamentales como axiomas, teoremas y demostraciones. Estos elementos
forman la base de la lógica y el razonamiento matemático, permitiendo
construir argumentos sólidos y verificar la veracidad de diversas
proposiciones.

- *Axiomas* Los axiomas son declaraciones o proposiciones que se aceptan
  como verdaderas sin necesidad de demostración. En la lógica y las
  matemáticas, los axiomas sirven como fundamentos sobre los cuales se
  construye todo el sistema teórico. No son arbitrarios; se eligen
  cuidadosamente para evitar contradicciones y para ser lo
  suficientemente potentes como para derivar teoremas relevantes. Un
  ejemplo clásico de axioma es el postulado de paralelas de Euclides,
  que afirma que por un punto exterior a una línea, se puede trazar
  una y solo una paralela a dicha línea.

- *Teoremas* Los teoremas son proposiciones que han sido demostradas
  como verdaderas dentro de un sistema lógico, utilizando axiomas y
  teoremas previamente establecidos. Los teoremas requieren una
  demostración rigurosa que muestre su veracidad. Un ejemplo famoso es
  el teorema de Pitágoras en la geometría euclidiana, el cual
  establece que en un triángulo rectángulo, el cuadrado de la
  hipotenusa (el lado opuesto al ángulo recto) es igual a la suma de
  los cuadrados de los otros dos lados.

- *Demostración* La demostración es el proceso mediante el cual se
  establece la verdad de un teorema. Utiliza una serie de pasos
  lógicos y deductivos, basados en axiomas y en teoremas ya
  demostrados, para llegar a la conclusión de que el teorema en
  cuestión es verdadero. Las demostraciones pueden adoptar diversas
  formas, como la demostración directa, donde se parte de los axiomas
  y se llega al teorema; la demostración por contradicción, donde se
  asume que el teorema es falso y se llega a un absurdo; y la
  demostración por inducción, útil especialmente para los teoremas que
  involucran números enteros.

Este proceso es fundamental en el razonamiento lógico y matemático, ya
que proporciona una base sólida para entender y verificar la verdad de
las proposiciones dentro de un marco teórico específico. A través de
los axiomas, teoremas y demostraciones, la lógica facilita la
construcción de conocimiento estructurado y coherente, permitiendo el
avance y la aplicación de las matemáticas y otras disciplinas que
dependen de la lógica formal.

**  El modelo de adquisición del conocimiento según la filosofía.
El modelo de adquisición del conocimiento según la filosofía aborda
cómo los seres humanos entienden, aprenden y conocen el mundo que les
rodea. Esta cuestión ha sido central en la filosofía desde sus
inicios, involucrando a filósofos de todas las épocas, desde los
antiguos hasta los contemporáneos. La filosofía del conocimiento, o
epistemología, estudia la naturaleza, el origen y los límites del
conocimiento. A lo largo de la historia, se han propuesto varios
modelos para explicar cómo adquirimos conocimiento, incluyendo el
empirismo, el racionalismo, el constructivismo, y la fenomenología,
entre otros.

- *Empirismo*: El empirismo sostiene que el conocimiento proviene de la
  experiencia sensorial. Según esta visión, todos los conceptos son
  derivados de la experiencia y la mente al nacer es una tabula rasa,
  una hoja en blanco sobre la cual la experiencia escribe. John Locke,
  George Berkeley y David Hume son algunos de los filósofos más
  destacados asociados con el empirismo. Por ejemplo, Locke argumentó
  que el conocimiento se construye a partir de ideas simples que se
  obtienen a través de la experiencia y que estas ideas simples se
  combinan para formar ideas complejas.

- *Racionalismo*: En contraposición al empirismo, el racionalismo
  argumenta que el conocimiento se adquiere principalmente a través de
  la razón y la intuición, más que a través de los sentidos. Los
  racionalistas creen en la existencia de ideas innatas, es decir,
  conocimientos que nacen con el individuo. René Descartes, Baruch
  Spinoza y Gottfried Wilhelm Leibniz son figuras clave del
  racionalismo. Descartes, por ejemplo, propuso el método de la duda
  sistemática y llegó a la conclusión de que la única certeza es
  "Cogito, ergo sum" ("Pienso, luego existo"), subrayando la primacía
  de la mente y la razón en la adquisición del conocimiento.

- *Constructivismo:* El constructivismo sostiene que el conocimiento se
  construye activamente por el cognoscente, no es simplemente una
  copia de la realidad. Esta teoría sugiere que los individuos
  construyen su conocimiento a través de la interacción con el entorno
  y mediante la reinterpretación de sus experiencias a la luz de sus
  propias creencias y antecedentes. Jean Piaget es uno de los teóricos
  más influyentes en esta área, argumentando que el desarrollo
  cognitivo del niño se produce a través de una serie de etapas y que
  el aprendizaje es un proceso de reorganización de estructuras
  mentales.

- *Fenomenología:* La fenomenología es un enfoque filosófico que
  enfatiza la experiencia subjetiva como la fuente principal del
  conocimiento. Fundada por Edmund Husserl, la fenomenología busca
  describir los fenómenos tal como se presentan a la conciencia, sin
  recurrir a teorías o interpretaciones previas. Este enfoque ha
  influenciado a muchos filósofos y teóricos, incluyendo a Martin
  Heidegger y Maurice Merleau-Ponty, y se centra en la comprensión de
  la experiencia vivida desde el punto de vista de la primera persona.

*Conclusión* El modelo de adquisición del conocimiento según la
filosofía no se limita a una única teoría o enfoque. En cambio,
refleja una rica diversidad de perspectivas sobre cómo los humanos
llegan a entender el mundo. Cada enfoque ofrece una visión diferente
sobre la naturaleza del conocimiento, cómo se adquiere, y los límites
de nuestra comprensión. La epistemología sigue siendo un campo de
estudio vibrante y en evolución, que continúa desafiando nuestras
concepciones sobre la mente, la realidad y la forma en que
interactuamos con el mundo que nos rodea.

**  El modelo cognoscitivo.
El modelo cognoscitivo es un enfoque teórico en la psicología que pone
énfasis en la comprensión de los procesos mentales internos que
subyacen a la percepción, el pensamiento, el aprendizaje, la memoria y
la resolución de problemas. Contrario a los modelos de conducta que se
enfocan únicamente en las respuestas observables a los estímulos, el
modelo cognoscitivo busca entender cómo las personas interpretan,
procesan y almacenan la información recibida del entorno. Este modelo
ha sido fundamental en el desarrollo de la psicología cognitiva, una
rama de la psicología que estudia los procesos mentales internos.


***  Fundamentos del Modelo Cognoscitivo

 El modelo cognoscitivo se basa en la premisa de que la mente funciona
 de manera similar a un ordenador: recibe datos (inputs), los procesa
 y luego produce respuestas (outputs). Este enfoque enfatiza la
 importancia de los procesos mentales internos y cómo estos influyen
 en la conducta. Los aspectos clave del modelo cognoscitivo incluyen:

 - *Percepción:* Cómo interpretamos y damos sentido a la información
   sensorial del mundo que nos rodea.
 - *Atención:* Cómo filtramos y seleccionamos información del entorno
   para procesarla más a fondo.
 - *Memoria:* Cómo almacenamos y recuperamos información. La memoria se
   considera en diferentes formatos, como la memoria a corto plazo (o
   memoria de trabajo) y la memoria a largo plazo.
 - *Pensamiento:* Cómo solucionamos problemas, tomamos decisiones y
   ejecutamos el razonamiento lógico.
 - *Lenguaje:* Cómo utilizamos el lenguaje para pensar, comunicarnos y entender el mundo.

 
***  Aplicaciones del Modelo Cognoscitivo

 El modelo cognoscitivo ha tenido aplicaciones extensas en varios campos, incluyendo:

 - *Educación:* Desarrollo de estrategias de enseñanza basadas en cómo
   los estudiantes procesan y recuerdan la información.
 - *Terapia Cognitiva:* En psicología clínica, se utiliza para tratar
   trastornos como la depresión y la ansiedad, ayudando a los
   pacientes a reconocer y cambiar patrones de pensamiento
   distorsionados.
 - *Diseño de Interfaces de Usuario:* En la tecnología de la
   información, se aplica para crear sistemas que se alineen mejor con
   los procesos cognitivos humanos, haciendo que las interfaces sean
   más intuitivas.

*** Críticas y Limitaciones

A pesar de su influencia y aplicabilidad, el modelo cognoscitivo también ha enfrentado críticas, principalmente por:

 - *Reduccionismo:* Algunos críticos argumentan que reduce los procesos
   mentales complejos a simples mecanismos computacionales.
 - *Descuido de lo Emocional y lo Social:* Inicialmente, el modelo
   cognoscitivo fue criticado por ignorar cómo las emociones y el
   contexto social afectan el procesamiento cognitivo.

*** Evolución y Expansión
 En respuesta a estas críticas, el modelo cognoscitivo ha evolucionado
 para incorporar aspectos emocionales y sociales en el estudio de los
 procesos mentales. Esto ha llevado al desarrollo de subcampos como la
 psicología cognitiva social y la neurociencia cognitiva, que exploran
 la interacción entre cognición, emoción y contextos sociales.

***  Conclusión
El modelo cognoscitivo ha sido fundamental en el avance de nuestra
comprensión de los procesos mentales y ha revolucionado la forma en
que los psicólogos abordan el estudio de la mente y la conducta. A
través de su aplicación en educación, terapia y tecnología, ha
demostrado ser una herramienta invaluable para mejorar diversos
aspectos de la vida humana. Con el tiempo, continúa adaptándose y
expandiéndose para incluir una comprensión más holística de la
cognición humana.


**  El modelo del agente inteligente, Sistemas Multi Agentes, Sistemas Ubicuos.
El modelo del agente inteligente, los Sistemas Multi-Agente (SMA) y
los Sistemas Ubicuos son conceptos fundamentales en el ámbito de la
inteligencia artificial (IA) y las ciencias de la computación, que
tienen aplicaciones en una amplia gama de dominios, desde la
automatización del hogar hasta la manufactura avanzada y los entornos
de trabajo colaborativos. A continuación, se desarrolla cada uno de
estos conceptos para proporcionar una comprensión clara de su
significado, funcionamiento y aplicaciones.

***  El Modelo del Agente Inteligente

 Un agente inteligente es una entidad autónoma capaz de percibir su
 entorno a través de sensores y actuar en ese entorno utilizando
 actuadores para lograr ciertos objetivos o maximizar una medida de
 rendimiento. Los agentes inteligentes pueden ser simples, como un
 termostato programado para mantener una temperatura específica, o
 complejos, como un robot autónomo explorando Marte. La inteligencia
 de un agente se refleja en su capacidad para tomar decisiones
 adecuadas en función de sus percepciones y los objetivos
 establecidos.

 
***  Sistemas Multi-Agente (SMA)

Los Sistemas Multi-Agente son sistemas compuestos por varios agentes
inteligentes que interactúan entre sí dentro de un entorno. Estos
agentes pueden cooperar, competir o negociar para lograr sus
objetivos individuales o colectivos. Los SMA son especialmente útiles
para resolver problemas que son demasiado complejos o grandes para
ser abordados por un único agente. Los ejemplos de aplicaciones
incluyen la simulación de sistemas sociales, la optimización de
procesos de manufactura, la gestión de redes de transporte y los
juegos serios para la educación y la formación.

 
***  Sistemas Ubicuos

Los Sistemas Ubicuos, también conocidos como computación ubicua, se
refieren a la integración de la computación en el entorno cotidiano
de manera que los dispositivos computacionales estén disponibles en
todo momento y lugar, pero de manera invisible para el usuario. Estos
sistemas se caracterizan por su capacidad para proporcionar servicios
y soporte de manera proactiva y context-aware, es decir, adaptando su
comportamiento según el contexto del usuario. Los ejemplos incluyen
casas inteligentes que ajustan automáticamente la iluminación y la
temperatura, ciudades inteligentes con gestión avanzada del tráfico y
sistemas de alerta temprana, y dispositivos personales de salud que
monitorizan constantemente el bienestar del usuario.

 
***  Integración y Aplicaciones
La integración de estos conceptos permite el desarrollo de soluciones
innovadoras a problemas complejos. Por ejemplo, un SMA puede ser
desplegado en un entorno ubicuo para gestionar de manera eficiente y
adaptativa los recursos energéticos de una ciudad inteligente. Los
agentes inteligentes, actuando dentro de este sistema, pueden
monitorear el consumo de energía en tiempo real, predecir la demanda
futura y ajustar de manera proactiva la distribución de energía para
maximizar la eficiencia y minimizar los costos.

*** Conclusión
El modelo del agente inteligente, los Sistemas Multi-Agente y los
Sistemas Ubicuos representan áreas clave en la investigación y
aplicación de la inteligencia artificial y la computación. Al combinar
la autonomía y la capacidad de toma de decisiones de los agentes
inteligentes con la cooperación y la interacción de los SMA, y al
integrar estos sistemas en el entorno cotidiano a través de la
computación ubicua, es posible desarrollar soluciones avanzadas y
adaptativas para una amplia gama de desafíos en la sociedad moderna.

**  El papel de la heurística.

*Búsqueda Heurística:* En la búsqueda heurística, las técnicas
heurísticas se utilizan para acelerar el proceso de búsqueda de
soluciones, especialmente en problemas de búsqueda en espacios de
estados grandes, como los encontrados en la planificación, juegos de
estrategia, y la resolución de puzzles. Algoritmos como A* y sus
variantes utilizan funciones heurísticas para estimar el costo del
camino más corto desde un nodo dado hasta el objetivo, priorizando la
exploración de caminos que parecen más prometedores.

- *Optimización* Las heurísticas también son cruciales en problemas de
  optimización, donde se busca la mejor solución de entre un conjunto
  de soluciones posibles. Técnicas como la búsqueda tabú, algoritmos
  genéticos y el recocido simulado son ejemplos de métodos heurísticos
  que exploran el espacio de soluciones de manera estratégica para
  encontrar soluciones óptimas o cercanas al óptimo en un tiempo
  razonable.

- *Toma de Decisiones*: En la toma de decisiones, las heurísticas ayudan
  a simplificar los procesos de evaluación y elección al reducir la
  complejidad de las decisiones y la cantidad de información a
  considerar. Esto es especialmente útil en la IA para juegos, donde
  los agentes deben tomar decisiones rápidas y efectivas en entornos
  competitivos con información incompleta.

- *Diseño de Algoritmos*: Las heurísticas son fundamentales en el diseño
  de algoritmos para el procesamiento de lenguaje natural, visión por
  computadora, y sistemas de recomendación, donde guían el análisis y
  la interpretación de datos complejos o no estructurados para
  realizar tareas como la clasificación, el reconocimiento de patrones
  y la predicción.

*** Ventajas y Limitaciones
Las principales ventajas de utilizar heurísticas en la IA incluyen la
capacidad de encontrar soluciones de manera más rápida y eficiente que
los métodos exhaustivos, especialmente en problemas complejos o de
gran escala. Sin embargo, el uso de heurísticas también tiene sus
limitaciones, ya que la solución encontrada no siempre es la óptima, y
la calidad de la solución puede depender significativamente de la
elección de la función heurística.

- *Conclusión*: La heurística es una herramienta invaluable en la
  inteligencia artificial, permitiendo el desarrollo de sistemas y
  algoritmos que pueden operar efectivamente en entornos complejos y
  bajo restricciones de tiempo o recursos. Aunque la elección y el
  diseño de funciones heurísticas adecuadas pueden ser desafiantes, su
  aplicación sigue siendo esencial para el avance y la eficacia de la
  IA en una amplia gama de aplicaciones prácticas.



*** Algoritmos de exploración de alternativas.



*** Algoritmo A*

El *algoritmo A** es uno de los algoritmos de búsqueda más populares
y eficientes para encontrar el camino más corto entre dos puntos,
especialmente en grafos o espacios de búsqueda con grandes cantidades
de nodos, como mapas o rejillas de nodos (grids). Es muy utilizado en
problemas de búsqueda de rutas, por ejemplo, en videojuegos y sistemas
de navegación.

**** *Conceptos Básicos*

A* combina las ventajas de los algoritmos de búsqueda de costo
uniforme (como Dijkstra) y búsqueda heurística (como la búsqueda en
anchura) para obtener un algoritmo que es tanto **óptimo** (encuentra
la mejor solución) como **completable** (lo hace en un tiempo
razonable).

A* utiliza una función de evaluación que combina dos valores clave:
- **g(n)**: El costo real desde el nodo inicial hasta el nodo actual \(n\).
- **h(n)**: Una estimación heurística del costo desde el nodo actual \(n\) hasta el nodo final (objetivo).

La función de evaluación se define como:
#+begin_src latex
f(n) = g(n) + h(n)
#+end_src
donde \(g(n)\) es el costo acumulado y \(h(n)\) es la estimación
heurística. La idea es expandir primero los nodos que parecen estar
más cerca del objetivo, mientras se asegura que el costo acumulado sea
mínimo.

*Características del Algoritmo A**
- *Óptimo*: Si la heurística es admisible, es decir, nunca sobreestima
  el costo restante (por ejemplo, si se usa la *distancia Manhattan*
  o la *distancia Euclidiana* como heurística en un espacio
  euclidiano), A* siempre encontrará el camino más corto.
- *Completo*: Si existe un camino desde el punto inicial al punto
  objetivo, A* lo encontrará, siempre que el espacio de búsqueda sea
  finito.

*Etapas del Algoritmo*

1. *Inicialización*:
   - Se parte de un nodo inicial (el punto de partida) y se agrega a una estructura de datos (generalmente una **cola de prioridad**).
   - Cada nodo tiene un valor de \(f(n)\) calculado como la suma de \(g(n)\) y \(h(n)\), donde \(g(n)\) es 0 para el nodo inicial y \(h(n)\) es el valor heurístico estimado.

2. **Expansión de Nodos**:
   - En cada paso, el algoritmo extrae el nodo con el menor valor \(f(n)\) de la cola de prioridad.
   - Luego, el algoritmo **expande** este nodo, es decir, explora todos sus nodos vecinos.
   - Para cada vecino, se calcula el nuevo costo \(g(n)\) como la distancia acumulada desde el nodo inicial y se actualiza el valor de \(f(n)\).

3. **Actualización de Nodos Vecinos**:
   - Si un vecino aún no ha sido explorado, o si se ha encontrado un camino más corto a dicho vecino, se actualizan sus valores de \(g(n)\) y \(f(n)\), y se agrega a la cola de prioridad para ser evaluado más adelante.

4. **Heurística**:
   - La heurística \(h(n)\) es crucial para guiar la búsqueda hacia el objetivo. Una **buena heurística** hace que A* sea más eficiente, ya que prioriza la expansión de los nodos más prometedores.
   - Las heurísticas comunes incluyen la **distancia Manhattan** (para cuadrículas donde solo se puede mover en horizontal o vertical) y la **distancia Euclidiana** (para espacios donde se puede mover en diagonal).

5. **Terminación**:
   - El algoritmo termina cuando se extrae el nodo objetivo (el nodo final) de la cola de prioridad, lo que significa que se ha encontrado el camino más corto.

6. **Reconstrucción del Camino**:
   - Una vez que se ha encontrado el nodo final, el camino se reconstruye retrocediendo desde el nodo final al nodo inicial a través de los nodos predecesores.

**** *Pseudocódigo*

Pseudocódigo simplificado del algoritmo A*:

#+begin_src python
Function A*(inicio, objetivo)
    crear una cola de prioridad `open_set`
    agregar `inicio` a `open_set` con f(inicio) = h(inicio)
    
    g_score[inicio] = 0
    f_score[inicio] = h(inicio)  // Heurística desde inicio hasta objetivo

    While `open_set` no esté vacío:
        current = nodo en `open_set` con menor valor de f_score
        If current == objetivo:
            return reconstruir_camino(current)  // Camino encontrado

        quitar current de `open_set`

        For cada vecino de current:
            tentative_g_score = g_score[current] + costo para moverse a vecino
            
            If tentative_g_score < g_score[vecino]:  // Mejor camino encontrado
                came_from[vecino] = current
                g_score[vecino] = tentative_g_score
                f_score[vecino] = g_score[vecino] + h(vecino, objetivo)
                
                If vecino no está en `open_set`:
                    agregar vecino a `open_set`

    Return fracaso  // Si no se encuentra un camino
#+end_src

*Ejemplo de Heurística: Distancia Manhattan*

Si se está trabajando en una cuadrícula donde solo se permiten movimientos horizontales y verticales, la **distancia Manhattan** es una heurística común:

\(h(n) = |x1 - x2| + |y1 - y2|\)

donde \((x1, y1)\) son las coordenadas del nodo actual \(n\) y \((x2, y2)\) son las coordenadas del nodo objetivo.

*Complejidad del Algoritmo*

- *Complejidad espacial*: A* puede consumir bastante memoria, ya que mantiene una lista de nodos abiertos y cerrados (nodos que han sido evaluados).
- *Complejidad temporal*: En el peor de los casos, A* tiene una complejidad de tiempo de \(O(b^d)\), donde \(b\) es el factor de ramificación (número promedio de vecinos por nodo) y \(d\) es la profundidad del camino más corto.

*Ventajas de A**

1. *Eficiente*: Encuentra el camino más corto de manera eficiente si la heurística es adecuada.
2. *Óptimo*: Siempre encuentra el camino más corto si la heurística es admisible.
3. *Flexible*: Funciona en muchos tipos de grafos y mapas.

*Desventajas de A**
1. *Costoso en memoria*: Puede requerir mucho espacio, especialmente en problemas de gran escala.
2. *Dependencia de la heurística*: Si la heurística no es adecuada, el rendimiento de A* puede degradarse significativamente.

*Aplicaciones*

- *Videojuegos*: Usado en motores de IA para encontrar rutas en mapas grandes, por ejemplo, en juegos de estrategia.
- *Sistemas de navegación*: GPS y otros sistemas de navegación utilizan variantes de A* para encontrar rutas óptimas.
- *Robótica*: En sistemas de planificación de rutas para robots autónomos.


*Visualización de Nodos, Cascaron A** 

#+BEGIN_SRC python 
import pygame

# Configuraciones iniciales
ANCHO_VENTANA = 800
VENTANA = pygame.display.set_mode((ANCHO_VENTANA, ANCHO_VENTANA))
pygame.display.set_caption("Visualización de Nodos")

# Colores (RGB)
BLANCO = (255, 255, 255)
NEGRO = (0, 0, 0)
GRIS = (128, 128, 128)
VERDE = (0, 255, 0)
ROJO = (255, 0, 0)
NARANJA = (255, 165, 0)
PURPURA = (128, 0, 128)

class Nodo:
    def __init__(self, fila, col, ancho, total_filas):
        self.fila = fila
        self.col = col
        self.x = fila * ancho
        self.y = col * ancho
        self.color = BLANCO
        self.ancho = ancho
        self.total_filas = total_filas

    def get_pos(self):
        return self.fila, self.col

    def es_pared(self):
        return self.color == NEGRO

    def es_inicio(self):
        return self.color == NARANJA

    def es_fin(self):
        return self.color == PURPURA

    def restablecer(self):
        self.color = BLANCO

    def hacer_inicio(self):
        self.color = NARANJA

    def hacer_pared(self):
        self.color = NEGRO

    def hacer_fin(self):
        self.color = PURPURA

    def dibujar(self, ventana):
        pygame.draw.rect(ventana, self.color, (self.x, self.y, self.ancho, self.ancho))

def crear_grid(filas, ancho):
    grid = []
    ancho_nodo = ancho // filas
    for i in range(filas):
        grid.append([])
        for j in range(filas):
            nodo = Nodo(i, j, ancho_nodo, filas)
            grid[i].append(nodo)
    return grid

def dibujar_grid(ventana, filas, ancho):
    ancho_nodo = ancho // filas
    for i in range(filas):
        pygame.draw.line(ventana, GRIS, (0, i * ancho_nodo), (ancho, i * ancho_nodo))
        for j in range(filas):
            pygame.draw.line(ventana, GRIS, (j * ancho_nodo, 0), (j * ancho_nodo, ancho))

def dibujar(ventana, grid, filas, ancho):
    ventana.fill(BLANCO)
    for fila in grid:
        for nodo in fila:
            nodo.dibujar(ventana)

    dibujar_grid(ventana, filas, ancho)
    pygame.display.update()

def obtener_click_pos(pos, filas, ancho):
    ancho_nodo = ancho // filas
    y, x = pos
    fila = y // ancho_nodo
    col = x // ancho_nodo
    return fila, col

def main(ventana, ancho):
    FILAS = 10
    grid = crear_grid(FILAS, ancho)

    inicio = None
    fin = None

    corriendo = True

    while corriendo:
        dibujar(ventana, grid, FILAS, ancho)
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                corriendo = False

            if pygame.mouse.get_pressed()[0]:  # Click izquierdo
                pos = pygame.mouse.get_pos()
                fila, col = obtener_click_pos(pos, FILAS, ancho)
                nodo = grid[fila][col]
                if not inicio and nodo != fin:
                    inicio = nodo
                    inicio.hacer_inicio()

                elif not fin and nodo != inicio:
                    fin = nodo
                    fin.hacer_fin()

                elif nodo != fin and nodo != inicio:
                    nodo.hacer_pared()

            elif pygame.mouse.get_pressed()[2]:  # Click derecho
                pos = pygame.mouse.get_pos()
                fila, col = obtener_click_pos(pos, FILAS, ancho)
                nodo = grid[fila][col]
                nodo.restablecer()
                if nodo == inicio:
                    inicio = None
                elif nodo == fin:
                    fin = None

    pygame.quit()

main(VENTANA, ANCHO_VENTANA)

#+END_SRC

#+RESULTS:
: None


**** *Tutorial de Pygame: Visualización de Nodos, Cascaron A**

Este programa utiliza la biblioteca Pygame para crear una cuadrícula
interactiva donde puedes definir un nodo de inicio, un nodo final, y
establecer paredes. Es una base para implementar algoritmos como A*
para la búsqueda de caminos.

***** 1. Importar Pygame

El primer paso es importar Pygame, que es una biblioteca para la creación de videojuegos en Python:

#+begin_src python
import pygame
#+end_src

***** 2. Configuraciones iniciales
Se configuran la ventana y los colores que se usarán en la
cuadrícula. `ANCHO_VENTANA` define el tamaño de la ventana, y
`VENTANA` se usa para crear la ventana donde se dibujarán los
elementos:

#+begin_src python
ANCHO_VENTANA = 800
VENTANA = pygame.display.set_mode((ANCHO_VENTANA, ANCHO_VENTANA))
pygame.display.set_caption("Visualización de Nodos")
#+end_src

- *`ANCHO_VENTANA`*: Define que la ventana será de 800x800 píxeles.
- *`pygame.display.set_mode`*: Crea la ventana con las dimensiones dadas.
- *`pygame.display.set_caption`*: Establece el título de la ventana.

***** 3. Definir colores en formato RGB
Definimos algunos colores que se usarán para los nodos y las paredes. Los colores se expresan en el formato RGB:

#+begin_src python
BLANCO = (255, 255, 255)
NEGRO = (0, 0, 0)
GRIS = (128, 128, 128)
VERDE = (0, 255, 0)
ROJO = (255, 0, 0)
NARANJA = (255, 165, 0)
PURPURA = (128, 0, 128)
#+end_src

***** 4. Clase Nodo
La clase `Nodo` representa cada celda de la cuadrícula. Cada nodo
tiene una posición (fila y columna), un color, y puede tener
diferentes estados (inicio, fin, pared, etc.):

#+begin_src python
class Nodo:
    def __init__(self, fila, col, ancho, total_filas):
        self.fila = fila
        self.col = col
        self.x = fila * ancho
        self.y = col * ancho
        self.color = BLANCO
        self.ancho = ancho
        self.total_filas = total_filas
#+end_src

Esta clase contiene varios métodos útiles:
- *`get_pos()`*: Devuelve la posición del nodo.
- *`es_pared()`, `es_inicio()`, `es_fin()`*: Determinan el estado del nodo.
- *`hacer_pared()`, `hacer_inicio()`, `hacer_fin()`*: Cambian el estado del nodo.
- *`dibujar()`*: Dibuja el nodo en la ventana.

***** 5. Crear la cuadrícula
El método `crear_grid` crea una lista de nodos organizados en una
cuadrícula. Cada nodo tiene un tamaño calculado dividiendo el ancho
total de la ventana por el número de filas.

#+begin_src python
def crear_grid(filas, ancho):
    grid = []
    ancho_nodo = ancho // filas
    for i in range(filas):
        grid.append([])
        for j in range(filas):
            nodo = Nodo(i, j, ancho_nodo, filas)
            grid[i].append(nodo)
    return grid
#+end_src

Este método crea una lista bidimensional que representa la cuadrícula de nodos.

***** 6. Dibujar la cuadrícula y los nodos
El método `dibujar_grid` dibuja las líneas que separan los nodos en la cuadrícula, mientras que `dibujar` dibuja cada nodo en la ventana.

#+begin_src python
def dibujar_grid(ventana, filas, ancho):
    ancho_nodo = ancho // filas
    for i in range(filas):
        pygame.draw.line(ventana, GRIS, (0, i * ancho_nodo), (ancho, i * ancho_nodo))
        for j in range(filas):
            pygame.draw.line(ventana, GRIS, (j * ancho_nodo, 0), (j * ancho_nodo, ancho))
#+end_src

- *`dibujar_grid`*: Dibuja las líneas horizontales y verticales para formar la cuadrícula.
- *`dibujar`*: Llama al método `dibujar()` de cada nodo para mostrar su estado (color).

***** 7. Obtener la posición del clic
El método `obtener_click_pos` convierte la posición en píxeles del ratón a la posición de la cuadrícula (fila y columna).

#+begin_src python
def obtener_click_pos(pos, filas, ancho):
    ancho_nodo = ancho // filas
    y, x = pos
    fila = y // ancho_nodo
    col = x // ancho_nodo
    return fila, col
#+end_src

Esto permite identificar qué nodo fue clicado con precisión.

***** 8. Ciclo principal
El ciclo principal del programa está en la función `main`, que
controla la interacción del usuario. Escucha eventos como el cierre de
la ventana o clics del ratón para definir los nodos de inicio, fin, y
las paredes.

#+begin_src python
def main(ventana, ancho):
    FILAS = 10
    grid = crear_grid(FILAS, ancho)

    inicio = None
    fin = None

    corriendo = True

    while corriendo:
        dibujar(ventana, grid, FILAS, ancho)
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                corriendo = False

            if pygame.mouse.get_pressed()[0]:  # Click izquierdo
                pos = pygame.mouse.get_pos()
                fila, col = obtener_click_pos(pos, FILAS, ancho)
                nodo = grid[fila][col]
                if not inicio and nodo != fin:
                    inicio = nodo
                    inicio.hacer_inicio()

                elif not fin and nodo != inicio:
                    fin = nodo
                    fin.hacer_fin()

                elif nodo != fin and nodo != inicio:
                    nodo.hacer_pared()

            elif pygame.mouse.get_pressed()[2]:  # Click derecho
                pos = pygame.mouse.get_pos()
                fila, col = obtener_click_pos(pos, FILAS, ancho)
                nodo = grid[fila][col]
                nodo.restablecer()
                if nodo == inicio:
                    inicio = None
                elif nodo == fin:
                    fin = None

    pygame.quit()
#+end_src

- *Interacción del ratón*:
  - *Clic izquierdo*: Define el nodo de inicio, el nodo final, o establece una pared.
  - *Clic derecho*: Restablece el nodo (lo borra).

Finalmente, la función `pygame.quit()` se encarga de cerrar correctamente el programa.



**** Fuentes sobre el Algoritmo A*

1. [[https://en.wikipedia.org/wiki/A*_search_algorithm][Wikipedia: A* Search Algorithm]] 
2. [[https://theory.stanford.edu/~amitp/GameProgramming/AStarComparison.html][Stanford University: Introduction to A*]] 
3. [[https://aquariusai.ca/a-star-algorithm/][Aquarius AI: A* Algorithm in AI]] 
4. [[https://brilliant.org/wiki/a-star-search/][Brilliant Math & Science: A* Search Algorithm]] 


*** Algoritmos de búsqueda local.
  
   



* Espacio de estados

Muchos de los problemas que pueden ser resueltos aplicando técnicas de
inteligencia artificial se modelan en forma simbólica y discreta
definiendo las configuraciones posibles del universo estudiado.  El
problema se plantea entoces en términos de encontrar una configuración
objetivo a partir de una configuración inicial dada, aplicando
transformaciones válidas según el modelo del universo.  La respuesta
es la secuencia de transformaciones cuya aplicación succesiva lleva a
la configuración deseada.

Los ejemplos más carácteristicos de esta categoría de problemas son
los juegos (son universos restringidos fáciles de modelar). En un
juego, las configuraciones del universo corresponden directamente a
las configuraciones del tablero. Cada configuración es un estado que
puede ser esquematizado gráficamente y representado en forma
simbólica. Las transformaciones permitidas corresponden a las reglas o
movidas del juego, formalizadas como transiciones de estado.

Entonces, para plantear formalmente un problema, se requiere precisar
una representación simbólica de los estados y definir reglas del tipo
condición acción para cada una de las transiciones válidas dentro del
universo modelado. La acción de una regla indica como modificar el
estado actual para generar un nuevo estado.  La condición impone
restricciones sobre la aplicabilidad de la regla según el estado
actual, el estado generado o la historia completa del proceso de
solución.

El espacio de estados de un juego es un grafo cuyos nodos representan
las configuraciones alcanzables (los estados válidos) y cuyos arcos
explicitan las movidas posibles (las transiciones de estado).  En
principio, se puede construir cualquier espacio de estados partiendo
del estado inicial, aplicando cada una de las reglas para generar los
sucesores immediatos, y así succesivamente con cada uno de los nuevos
estados generados (en la práctica, los espacios de estados suelen ser
demasiado grandes para explicitarlos por completo).

Cuando un problema se puede representar mediante un espacio de
estados, la solución computacional correspende a encontrar un camino
desde el estado inicial a un estado objetivo.

** Ejemplo de espacio de estados

*** Descripción del problema  

 Un arriero se encuentra en el borde de un rio llevando un puma, una
 cabra y una lechuga.  Debe cruzar a la otra orilla por medio de un
 bote con capacidad para dos (el arriero y alguna de sus
 pertenecias). La dificultad es que si el puma se queda solo con la
 cabra la devorará, y lo mismo sucederá si la cabra se queda sola con
 la lechuga. ¿Cómo cruzar sin perder ninguna pertenencia?

*** Representación de las configuraciones del universo del problema:

 Basta precisar la situación antes o después de cruzar. El arriero y
 cada una de sus pertenencias tienen que estar en alguna de las dos
 orillas. La representación del estado debe entonces indicar en que
 lado se encuentra cada uno de ellos. Para esto se puede utilizar un
 término simbólico con la siguiente sintáxis: estado(A,P,C,L), en que
 A, P, C y L son variables que representan, respectivamente, la
 posición del arriero, el puma, la cabra y la lechuga. Las variables
 pueden tomar dos valores: i y d, que simbolizan respectivamente el
 borde izquierdo y el borde derecho del rio. Por convención se elige
 partir en el borde izquierdo. El estado inicial es entonces
 estado(i,i,i,i). El estado objetivo es estado(d,d,d,d).

*** Definición de las reglas de transición:
 El arriero tiene cuatro acciones posibles: cruzar solo, cruzar con el
 puma, cruzar con la cabra y cruzar con la lechuga. Estas acciones
 están condicionadas a que ambos pasajeros del bote estén en la misma
 orilla y a que no queden solos el puma con la cabra o la cabra con la
 lechuga. El estado resultante de una acción se determina
 intercambiando los valores i y d para los pasajeros del bote.

*** Generación del espacio de estados
 En este ejemplo se puede explicitar todo el espacio de estados (el
 número de configuraciones está acotado por 24).

 #+ATTR_ORG: :width 200
 [[file:img/sd.png]]



*** Problemas de los Canibales y Monjes

 Se tienen 3 monjes y 3 caníbales en el margen Oeste de un río. Existe
 una canoa con capacidad para dos personas como máximo. Se desea que
 los seis pasen al margen Este del río, pero hay que considerar que no
 debe haber más caníbales que monjes en ningún sitio porque entonces
 los caníbales se comen a los monjes. Además, la canoa siempre debe ser
 conducida por alguien.\\


*** El espacio de estados está definido por

{(Mo, Co, Me, Ce, C) / Mo es el número de monjes en el margen oeste con
0<=Mo<=3\\
 AND Co es el número de caníbales en el margen oeste con 0<=Co<=3
AND (Co<=Mo OR Mo=0)\\
 AND Me es el número de monjes en el margen este con
0<=Me<=3 \\
AND Ce es el número de caníbales en el margen este con 0<=Ce<=3
AND (Ce<=Me OR Me=0) AND Co+Ce=3 AND Mo+Me=3 AND C = [E|O] es el margen dónde está la canoa}\\

El estado inicial es (3,3,0,0,O)

El estado final es (0,0,3,3,E)

Las reglas que se pueden aplicar son:

- Viajan un monje y un caníbal de O a E:
  Si (Mo, Co, Me, Ce, O) AND Mo>=1 AND Co>=1 AND Ce+1<=Me+1 => (Mo-1, Co-1, Me+1, Ce+1, E)
- Viajan un monje y un caníbal de E a O:
  Si (Mo, Co, Me, Ce, E) AND Me>=1 AND Ce>=1 AND Co+1<=Mo+1=> (Mo+1, Co+1, Me-1, Ce-1,O)

- Viajan dos monjes de O a E:
   Si (Mo, Co, Me, Ce, O) AND Mo>=2 AND (Mo-2=0 OR Co<=Mo-2) AND Ce<=Me+2=> (Mo-2, Co, Me+2, Ce, E)

- Viajan dos monjes de E a O:
  Si (Mo, Co, Me, Ce, E) AND Me>=2 AND (Me-2=0 OR Ce<=Me-2) AND Co<=Mo+2 => (Mo+2, Co, Me-2, Ce, O)

- Viajan dos caníbales de O a E:
  Si (Mo, Co, Me, Ce, O) AND Co>=2 AND (Me=0 OR Ce+2<=Me) => (Mo, Co-2, Me, Ce+2, E)

- Viajan dos caníbales de E a O:
  Si (Mo, Co, Me, Ce, E) AND Ce>=2 AND (Mo=0 OR Co+2<=Mo) => (Mo, Co+2, Me, Ce-2, O)

- Viaja un monje de O a E:
  Si (Mo, Co, Me, Ce, O) AND Mo>=1 AND (Mo-1=0 OR Co<=Mo-1) AND Ce<= Me+1 => (Mo-1, Co, Me+1, Ce, E)

- Viaja un monje de E a O:
  Si (Mo, Co, Me, Ce, E) AND Me>=1 AND (Me-1=0 OR Ce<=Me-1) AND Co<=Mo+1 => (Mo+1, Co, Me-1, Ce,O)

- Viaja un caníbal de O a E:
  Si (Mo, Co, Me, Ce, O) AND Co>=1 AND (Me=0 OR Ce+1<=Me) => (Mo, Co-1, Me, Ce+1, E)

- Viaja un caníbal de E a O:
  Si (Mo, Co, Me, Ce, O) AND Ce>=1 AND (Mo=0 OR Co+1<=Mo) => (Mo, Co+1, Me, Ce-1, E)


Nota: En referencia a la regla 3 la condición Ce<=Me+2 puede intuirse
como redundante. Esta condición no se cumple sólo en el caso Ce=3 y
Me=0. Pese a

que es un estado que pertenece al espacio de estados válidos, podemos
intuir que nunca se llega a tener 3 caníbales y ningún monje del lado
Este y la barca del lado Oeste. De todas maneras sólo se puede
eliminar si podemos demostrar formalmente la imposibilidad de esta
situación.

Un pasaje de estados para ir de (3,3,0,0,O) a (0,0,3,3,E) es el siguiente:

(3,3,0,0,O) => (3,1,0,2,E) => (3,2,0,1,O) => (3,0,0,3,E) => (3,1,0,2,O) =>
(1,1,2,2,E) => (2,2,1,1,O) => (0,2,3,1,E) => (0,3,3,0,O) => (0,1,3,2,E) =>
(0,2,3,1,O) =>(0,0,3,3,E)

** Representación de espacio de estados
 La primera pregunta es, como  

** El problema del n-Puzzle

*** Caracterización de las búsquedas ciegas. 
 La búsqueda ciega o no informada sólo utiliza información acerca de si
 un estado es o no objetivo para guiar su procesu de búsqueda.

 Los métodos de búsqueda ciega se pueden clasificar en dos grupos
 básicos:

 - *Métodos de búsqueda en anchura*: Son procedimientos de búsqueda nivel
   a nivel. Para cada uno de los nodos de un nivel se aplican todos los
   posibles operadores y no se expande ningún nodo de un nivel antes de
   haber expandido todos los del nivel anterior.

 - *Métodos de búsqueda en profundidad*: En estos procedimientos se realiza la búsqueda por
   una sola rama del árbol hasta encontrar una solución o hasta que se tome la decisión de
   terminar la búsqueda por esa dirección ( por no haber posibles operadores que aplicar sobre
   el nodo hoja o por haber alcanzado un nivel de profundidad muy grande ) . Si esto ocurre
   se produce una vuelta atrás ( backtracking ) y se sigue por otra rama hasta visitar todas
   las ramas del árbol si es necesario.


 A partir de los dos tipos de búsqueda anteriores surgió uno nuevo,
 llamado método de búsqueda por profundización iterativa. El algoritmo
 de búsqueda más representativo de esta nueva tendencia es el DFID
 acrónimo de su nombre en inglés (Depth-First Iterative-Deepening).


*** Caracterización de las búsquedas heurísticas.
  
 Las técnicas de búsqueda heurística se apoyan alc contrario de los
 métodos de búsqueda ciega se apoyan en información adicional para
 realizar su proceso de búsqueda. Para mejorar la eficiencia de la
 búsqueda, estos algoritmos hacen uso de una función que realiza una
 predicción del coste necesario para alcanzar la solución. La función
 que guía el proceso toma el nombre de función heurística.

 De todos los algoritmos de búsqueda heurística, uno destaca en
 especial: el A*. Este algoritmo, a pesar de haber sido creado entorno
 a los años 60, sigue en la actualidad siendo uno de los mas
 utilizados. Desafortunadamente, es ineficiente en cuanto al uso de
 memoria durante el proceso de búsqueda. Por ello, en las décadas de
 los 80 y 90, aparecieron algoritmos basados en el propio A*, pero que
 limitaban el uso de memoria. Dos de los algoritmos más representativos
 de esta última tendencia son el IDA* (Iterative-Deepening A*) y el
 SMA* (Simplified Memory-bounded A*).

* Técnicas de Búsqueda

** Solución de problemas con búsqueda.


La solución de problemas es fundamental para la mayoría de las
aplicaciones de IA; existen principalmente dos clases de problemas que
se pueden resolver mediante procesos computables: aquéllos en los que
se utiliza un algoritmo determinista que garantiza la solución al
problema y las tareas complejas que se resuelven con la búsqueda de
una solución; de ésta última clase de problemas se ocupa la IA.

La solución de problemas requiere dos consideraciones:

- Representación del problema en un espacio organizado.
- La capacidad de probar la existencia del estado objetivo en dicho espacio.

Las anteriores premisas se traducen en: la determinación del estado
objetivo y la determinación del camino óptimo guiado por este objetivo
a través de una o más transiciones dado un estado inicial

El espacio de búsqueda, se le conoce como una colección de estados.
En general los espacios de búsqueda en los problemas de IA no son completamente conocidos de forma a priori.
De lo anterior ‘resolver un problema de IA’ cuenta con dos fases:\\[0.5cm]

- La generación del espacio de estados
- La búsqueda del estado deseado en ese espacio.

Debido a que "todo el espacio de búsqueda" de un problema es muy
grande, puede causar un bloqueo de memoria, dejando muy poco espacio
para el proceso de búsqueda. Para solucionar esto, se expande el
espacio paso a paso, hasta encontrar el estado objetivo.


** Espacios de Estados

Muchos de los problemas que pueden ser resueltos aplicando técnicas de inteligencia artificial se modelan en forma simbólica y
discreta definiendo las configuraciones posibles del universo estudiado. El problema se plantea entonces en términos de encontrar una configuración objetivo a partir de una configuración inicial dada, aplicando transformaciones válidas según el modelo del universo. La respuesta es la secuencia de transformaciones cuya aplicación succesiva lleva a la configuración deseada.
Los ejemplos más carácteristicos de esta categoría de problemas son los juegos (son universos restringidos fáciles de modelar). En un juego, las configuraciones del universo corresponden directamente a las configuraciones del tablero. Cada configuración es un estado que puede ser esquematizado gráficamente y representado en forma simbólica. Las transformaciones permitidas corresponden a las reglas o movidas del juego, formalizadas como transiciones de estado.
Entonces, para plantear formalmente un problema, se requiere precisar una representación simbólica de los estados y definir reglas del tipo condición   acción para cada una de las transiciones válidas dentro del universo modelado. La acción de una regla indica como modificar el estado actual para generar un nuevo estado. La condición impone restricciones sobre la aplicabilidad de la regla según el estado actual, el estado generado o la historia completa del proceso de solución.
El espacio de estados de un juego es un grafo cuyos nodos representan las configuraciones alcanzables (los estados válidos) y cuyos arcos explicitan las movidas posibles (las transiciones de estado). En principio, se puede construir cualquier espacio de estados partiendo del estado inicial, aplicando cada una de las reglas para generar los sucesores immediatos, y así succesivamente con cada uno de los nuevos estados generados (en la práctica, los espacios de estados suelen ser demasiado grandes para explicitarlos por completo).
Cuando un problema se puede representar mediante un espacio de estados, la solución computacional correspende a encontrar un camino desde el estado inicial a un estado objetivo.

*** Deterministicos

El espacio de estados determinísticos contienen un único estado inicial y seguir la secuencia de estados para la solución. Los espacios de estados determinísticos son usados por los sistemas expertos.
Se puede describir asu vez, que un sistema es determinístico si, para un estado dado, al menos aplica una regla a él y de solo una manera.

*** No Deterministicos
El no determinístico contiene un amplio número de estados iniciales y sigue la secuencia de estados perteneciente al estado inicial del espacio. Son usados por sistemas de lógica difusa.
En otras palabras,  si más de una regla aplica a cualquier estado particular del sistema, o si una regla aplica a un estado particular del sistema en más de una manera, entonces el sistema es no determinístico.


** Métodos de Búsqueda

*** Primero en anchura (breadthfirst) 
En inglés, breadth-first search.
Si el conjunto open se maneja como una lista FIFO, es decir, como una cola, siempre se estará visitando primero los primeros estados en ser generados. El recorrido del espacio de estados se hace por niveles de profundidad.

#+BEGIN_SRC C
procedure Busqueda_en_amplitud {
   open ()[estado_inicial]
   closed () {}
   while (open no esta vacia) {
     remover el primer estado X de la lista open
     if (X es un estado objetivo) return exito
     else {
       generar el conjunto de sucesores del estado X
       agregar el estado X al conjunto closed
       eliminar sucesores que ya estan en open o en closed
       agregar el resto de los sucesores al final de open
     }
   }
   return fracaso
 }

#+END_SRC

Si el factor de ramificación es B y la profundidad a la cual se encuentra el estado objetivo más cercano es n, este algoritmo tiene una complejidad en tiempo y espacio de $O(B^n)$.
Contrariamente a la búsqueda en profundidad, la búsqueda en amplitud garantiza encontrar el camino más corto.  

*** Primero en profundidad (depthfirst).

En inglés, depth-first search.
Si el conjunto open se maneja como una lista LIFO, es decir, como un stack, siempre se estará
visitando primero los últimos estados en ser generados. Esto significa que si A genera B y C, y B
genera D, antes de visitar C se visita D, que está más alejado de la raiz A, o sea más profundo en
el árbol de búsqueda. El algoritmo tiene en este caso la tendencia de profundizar la búsqueda en
una rama antes de explorar ramas alternativas.

#+BEGIN_SRC C
procedure Busqueda_en_profundidad {
   open () [estado_inicial]
   closed () {}
   while (open no esta vacia) {
     remover el primer estado X de la lista open
     if (X es un estado objetivo) return exito
     else {
       generar el conjunto de sucesores del estado X
       agregar el estado X al conjunto closed
       eliminar sucesores que ya estan en open o en closed
       agregar el resto de los sucesores al principio de open
     }
   }
   return fracaso
 }


#+END_SRC

Considerando que la cantidad promedio de sucesores de los nodos visitados es B (llamado en inglés el
branching factor y en castellano el factor de ramificación), y suponiendo que la profundidad máxima alcanzada es n,
este algoritmo tiene una complejidad en tiempo de $O(B^n)$ y, si no se considera el conjunto closed, una complejidad en
espacio de O(B × n). En vez de usar el conjunto closed, el control de ciclos se puede hacer descartando aquellos estados que aparecen en el camino generado hasta el momento (basta que cada estado generado tenga un puntero a su padre).
El mayor problema de este algoritmo es que puede "perderse" en una rama sin encontrar la solución. Además, si se encuentra una solución no se puede garantizar que sea el camino más corto.

*** Búsqueda Heurística 
El algoritmo de búsqueda A* (pronunciado "A asterisco", "A estrella" o
"Astar" en inglés) se clasifica dentro de los algoritmos de búsqueda
en grafos de tipo heurístico o informado. Presentado por primera vez
en 1968 por Peter E. Hart, Nils J. Nilsson y Bertram Raphael, el
algoritmo A* encuentra, siempre y cuando se cumplan unas determinadas
condiciones, el camino de menor coste entre un nodo origen y uno
objetivo.\\

El problema de algunos algoritmos de búsqueda en grafos informados,
como puede ser el algoritmo voraz, es que se guían en exclusiva por la
función heurística, la cual puede no indicar el camino de coste más
bajo, o por el coste real de desplazarse de un nodo a otro (como los
algoritmos de escalada), pudiéndose dar el caso de que sea necesario
realizar un movimiento de coste mayor para alcanzar la solución. Es
por ello bastante intuitivo el hecho de que un buen algoritmo de
búsqueda informada debería tener en cuenta ambos factores, el valor
heurístico de los nodos y el coste real del recorrido.

Así, el algoritmo A* utiliza una función de evaluación
$f(n)=g(n)+h'(n)$, donde $h'(n)$ representa el valor heurístico del
nodo a evaluar desde el actual, n, hasta el final, y $g(n)$ $g(n)$, el
coste real del camino recorrido para llegar a dicho nodo, n, desde el
nodo inicial. A* mantiene dos estructuras de datos auxiliares, que
podemos denominar abiertos, implementado como una cola de prioridad
(ordenada por el valor $f(n)$ de cada nodo), y cerrados, donde se
guarda la información de los nodos que ya han sido visitados. En cada
paso del algoritmo, se expande el nodo que esté primero en abiertos, y
en caso de que no sea un nodo objetivo, calcula la $f(n)$ de todos sus
hijos, los inserta en abiertos, y pasa el nodo evaluado a cerrados.

El algoritmo es una combinación entre búsquedas del tipo primero en
anchura con primero en profundidad: mientras que $h'(n)$ tiende a
primero en profundidad, $g(n)$ tiende a primero en anchura. De este
modo, se cambia de camino de búsqueda cada vez que existen nodos más
prometedores.



**** Propiedades
Como todo algoritmo de búsqueda en amplitud, A* es un algoritmo
completo: en caso de existir una solución, siempre dará con ella.

Si para todo nodo n del grafo se cumple $g(n)=0$, nos encontramos ante
una búsqueda voraz. Si para todo nodo n del grafo se cumple $h(n)=0$,
A* se comporta como el algoritmo de Dijkstra.

Para garantizar la admisibilidad del algoritmo, la función $h(n)$
debe ser heurística admisible, esto es, que no sobrestime el coste
real de alcanzar el nodo objetivo, es decir, h(n) debe ser menor que
h*(n) para todo nodo no final.

Se garantiza que $h(n)$ es
consistente (o monótona), es decir, que para cualquier nodo
$n$ y cualquiera de sus sucesores, el coste estimado de
alcanzar el objetivo desde n no es mayor que el de alcanzar el sucesor
más el coste de alcanzar el objetivo desde el sucesor.



**** Complejidad 

La complejidad computacional del algoritmo está íntimamente
relacionada con la calidad de la heurística que se utilice en el
problema. En el caso peor, con una heurística de pésima calidad, la
complejidad será exponencial, mientras que en el caso mejor, con una
buena $h'(n)$, el algoritmo se
ejecutará en tiempo lineal. Para que esto último suceda, se debe
cumplir que

$$ h'(x)\leq g(y)-g(x)+h'(y)$$ donde h' es una heurística óptima para el problema,
como por ejemplo, el coste real de alcanzar el objetivo.


El espacio requerido por A* para ser ejecutado es su mayor
problema. Dado que tiene que almacenar todos los posibles siguientes
nodos de cada estado, la cantidad de memoria que requerirá será
exponencial con respecto al tamaño del problema. Para solucionar este
problema, se han propuesto diversas variaciones de este algoritmo,
como pueden ser RTA*, IDA* o SMA*.

** Satisfacción de restricciones.
 Los problemas pueden resolverse buscando en un espacio de estados, estos estados pueden evaluarse por heurísticas específicas para el dominio y probados para verificar si son estados meta.
 Los componentes del estado, son equivalentes a un grafo de restricciones, los cuales están compuestos de:\\[0.5cm]

 - *Variables*: Dominios (valores posibles para las variables).
 - *Restricciones* (binarias) entre las variables.


 Objetivo: encontrar un estado (una asignación completa de valores a las variables) Que satisface las restricciones.

 En los Problemas de Satisfacción de Restricciones (PSR), los estados y
 la prueba de meta siguen a una representación estándar, estructurada y
 muy simple.

 Ejemplos:

 - Crucigramas
 - Colorear mapas  

* Teoría de juegos.

Siendo una de las principales capacidades de la inteligencia humana su
capacidad para resolver problemas, así como la habilidad para analizar
los elementos esenciales de cada problema, abstrayéndolos, el
identificar las acciones que son necesarias para resolverlos y el
determinar cuál es la estrategia más acertada para atacarlos, son
rasgos fundamentales.

Podemos definir la resolución de problemas como el proceso que
partiendo de unos datos iníciales y utilizando un conjunto de
procedimientos escogidos, es capaz de determinar el conjunto de pasos
o elementos que nos llevan a lo que denominaremos una solución óptima
o semi-óptima de un problema de planificación, descubrir una
estrategia ganadora de un juego, demostrar un teorema, reconocer

Una imagen, comprender una oración o un texto son algunas de las
tareas que pueden concebirse como de resolución.

Una gran ventaja que nos proporciona la utilización de los juegos es
que a través de ellos es muy fácil medir el éxito o el fracaso, por lo
que podemos comprobar si las técnicas y algoritmos empleados son los
óptimos. En comparación con otras aplicaciones de inteligencia
artificial, por ejemplo comprensión del lenguaje, los juegos no
necesitan grandes cantidades de algoritmos. Los juegos más utilizados
son las damas y el ajedrez.



* Grafos

Un grafo es un conjunto de puntos (vértices) en el espacio, que están conectados
por un conjunto de líneas (aristas). Otros conceptos básicos son:
Dos vértices son adyacentes si comparten la misma arista.
Los extremos de una arista son los vértices que comparte dicha arista.
Un grafo se dice que es finito si su número de vértices es finito.

* Tipos de grafos

Existen dos tipos de grafos los no dirigidos y los dirigidos.

• *No dirigidos*: son aquellos en los cuales los lados no están orientados (No son
flechas). Cada lado se representa entre paréntesis, separando sus vértices por
comas, y teniendo en cuenta (Vi,Vj)=(Vj,Vi).

• *Dirigidos*: son aquellos en los cuales los lados están orientados (flechas).
Cada lado se representa entre ángulos, separando sus vértices por comas y
teniendo en cuenta <Vi ,Vj>!=<Vj ,Vi>. En grafos dirigidos, para cada lado <A,B>,
A, el cual es el vértice origen, se conoce como la cola del lado y B, el cual es
el vértice destino, se conoce como cabeza del lado.

* Machine Learning 


El aprendizaje automático o aprendizaje automatizado o aprendizaje de
máquinas (del inglés, machine learning) es el subcampo de las ciencias
de la computación y una rama de la inteligencia artificial, cuyo
objetivo es desarrollar técnicas que permitan que las computadoras
aprendan. Se dice que un agente aprende cuando su desempeño mejora con
la experiencia y mediante el uso de datos; es decir, cuando la
habilidad no estaba presente en su genotipo o rasgos de nacimiento.1​
"En el aprendizaje de máquinas un computador observa datos, construye
un modelo basado en esos datos y utiliza ese modelo a la vez como una
hipótesis acerca del mundo y una pieza de software que puede resolver
problemas".

En muchas ocasiones el campo de actuación del aprendizaje automático
se solapa con el de la estadística inferencial, ya que las dos
disciplinas se basan en el análisis de datos. Sin embargo, el
aprendizaje automático incorpora las preocupaciones de la complejidad
computacional de los problemas. Muchos problemas son de clase NP-hard,
por lo que gran parte de la investigación realizada en aprendizaje
automático está enfocada al diseño de soluciones factibles a esos
problemas. El aprendizaje automático también está estrechamente
relacionado con el reconocimiento de patrones. El aprendizaje
automático puede ser visto como un intento de automatizar algunas
partes del método científico mediante métodos matemáticos. Por lo
tanto es un proceso de inducción del conocimiento.

El aprendizaje automático tiene una amplia gama de aplicaciones,
incluyendo motores de búsqueda, diagnósticos médicos, detección de
fraude en el uso de tarjetas de crédito, análisis del mercado de
valores, clasificación de secuencias de ADN, reconocimiento del habla
y del lenguaje escrito, juegos y robótica.

** Tipos de Algoritmos

Los diferentes algoritmos de Aprendizaje Automático se agrupan en una
taxonomía en función de la salida de los mismos. Algunos tipos de
algoritmos son:

- *Aprendizaje supervisado* : El algoritmo produce una función que
  establece una correspondencia entre las entradas y las salidas
  deseadas del sistema. Un ejemplo de este tipo de algoritmo es el
  problema de clasificación, donde el sistema de aprendizaje trata de
  etiquetar (clasificar) una serie de vectores utilizando una entre
  varias categorías (clases). La base de conocimiento del sistema está
  formada por ejemplos de etiquetados anteriores. Este tipo de
  aprendizaje puede llegar a ser muy útil en problemas de
  investigación biológica, biología computacional y bioinformática.

- *Aprendizaje no supervisado*: Todo el proceso de modelado se lleva a
  cabo sobre un conjunto de ejemplos formado tan solo por entradas al
  sistema. No se tiene información sobre las categorías de esos
  ejemplos. Por lo tanto, en este caso, el sistema tiene que ser capaz
  de reconocer patrones para poder etiquetar las nuevas entradas.

- *Aprendizaje semisupervisado*: Este tipo de algoritmos combinan los
  dos algoritmos anteriores para poder clasificar de manera
  adecuada. Se tiene en cuenta los datos marcados y los no marcados.

- *Aprendizaje por refuerzo*: El algoritmo aprende observando el mundo
  que le rodea. Su información de entrada es el feedback o
  retroalimentación que obtiene del mundo exterior como respuesta a
  sus acciones. Por lo tanto, el sistema aprende a base de
  ensayo-error. El aprendizaje por refuerzo es el más general entre
  las tres categorías. En vez de que un instructor indique al agente
  qué hacer, el agente inteligente debe aprender cómo se comporta el
  entorno mediante recompensas (refuerzos) o castigos, derivados del
  éxito o del fracaso respectivamente. El objetivo principal es
  aprender la función de valor que le ayude al agente inteligente a
  maximizar la señal de recompensa y así optimizar sus políticas de
  modo a comprender el comportamiento del entorno y a tomar buenas
  decisiones para el logro de sus objetivos formales.  Los principales
  algoritmos de aprendizaje por refuerzo se desarrollan dentro de los
  métodos de resolución de problemas de decisión finitos de Markov,
  que incorporan las ecuaciones de Bellman y las funciones de
  valor. Los tres métodos principales son: la Programación Dinámica,
  los métodos de Monte Carlo y el aprendizaje de Diferencias
  Temporales. Entre las implementaciones desarrolladas está AlphaGo,
  un programa de IA desarrollado por Google DeepMind para jugar el
  juego de mesa Go. En marzo de 2016 AlphaGo le ganó una partida al
  jugador profesional Lee Se-Dol que tiene la categoría noveno dan y
  18 títulos mundiales. Entre los algoritmos que utiliza se encuentra
  el árbol de búsqueda Monte Carlo, también utiliza aprendizaje
  profundo con redes neuronales. Puede ver lo ocurrido en el
  documental de Netflix “AlphaGo”.

- *Transducción*: Similar al aprendizaje supervisado, pero no construye
  de forma explícita una función. Trata de predecir las categorías de
  los futuros ejemplos basándose en los ejemplos de entrada, sus
  respectivas categorías y los ejemplos nuevos al sistema.

- *Aprendizaje multi-tarea*: Métodos de aprendizaje que usan
  conocimiento previamente aprendido por el sistema de cara a
  enfrentarse a problemas parecidos a los ya vistos. El análisis
  computacional y de rendimiento de los algoritmos de aprendizaje
  automático es una rama de la estadística conocida como teoría
  computacional del aprendizaje. El aprendizaje automático las
  personas lo llevamos a cabo de manera
  automática ya que es un proceso tan sencillo para nosotros que ni nos
  damos cuenta de cómo se realiza y todo lo que implica. Desde que
  nacemos hasta que morimos los seres humanos llevamos a cabo diferentes
  procesos, entre ellos encontramos el de aprendizaje por medio del cual
  adquirimos conocimientos, desarrollamos habilidades para analizar y
  evaluar a través de métodos y técnicas así como también por medio de
  la experiencia propia. Sin embargo, a las máquinas hay que indicarles
  cómo aprender, ya que si no se logra que una máquina sea capaz de
  desarrollar sus habilidades, el proceso de aprendizaje no se estará
  llevando a cabo, sino que solo será una secuencia repetitiva.

** Técnicas de clasificación

- *Árboles de decisiones*: Este tipo de aprendizaje usa un árbol de
  decisiones como modelo predictivo. Se mapean observaciones sobre un
  objeto con conclusiones sobre el valor final de dicho objeto. Los
  árboles son estructuras básicas en la informática. Los árboles de
  atributos son la base de las decisiones. Una de las dos formas
  principales de árboles de decisiones es la desarrollada por Quinlan
  de medir la impureza de la entropía en cada rama, algo que primero
  desarrolló en el algoritmo ID3 y luego en el C4.5. Otra de las
  estrategias se basa en el índice GINI y fue desarrollada por
  Breiman, Friedman et alia. El algoritmo de CART es una
  implementación de esta estrategia.5​

- *Reglas de asociación*: Los algoritmos de reglas de asociación
  procuran descubrir relaciones interesantes entre variables. Entre
  los métodos más conocidos se hallan el algoritmo a priori, el
  algoritmo Eclat y el algoritmo de Patrón Frecuente.

- *Algoritmos genéticos*: Los algoritmos genéticos son procesos de
  búsqueda heurística que simulan la selección natural. Usan métodos
  tales como la mutación y el cruzamiento para generar nuevas clases
  que puedan ofrecer una buena solución a un problema dado.

- *Redes neuronales artificiales*: Las redes de neuronas artificiales
  (RNA) son un paradigma de aprendizaje automático inspirado en las
  neuronas de los sistemas nerviosos de los animales. Se trata de un
  sistema de enlaces de neuronas que colaboran entre sí para producir
  un estímulo de salida. Las conexiones tienen pesos numéricos que se
  adaptan según la experiencia. De esta manera, las redes neurales se
  adaptan a un impulso y son capaces de aprender. La importancia de
  las redes neurales cayó durante un tiempo con el desarrollo de los
  vectores de soporte y clasificadores lineales, pero volvió a surgir
  a finales de la década de 2000 con la llegada del aprendizaje
  profundo.

- *Máquinas de vectores de soporte*: Las MVS son una serie de métodos
  de aprendizaje supervisado usados para clasificación y
  regresión. Los algoritmos de MVS usan un conjunto de ejemplos de
  entrenamiento clasificado en dos categorías para construir un modelo
  que prediga si un nuevo ejemplo pertenece a una u otra de dichas
  categorías.

- *Algoritmos de agrupamiento* El análisis por agrupamiento
  (clustering en inglés) es la clasificación de observaciones en
  subgrupos —clusters— para que las observaciones en cada grupo se
  asemejen entre sí según ciertos criterios. Las técnicas de
  agrupamiento hacen inferencias diferentes sobre la estructura de los
  datos; se guían usualmente por una medida de similitud específica y
  por un nivel de compactamiento interno (similitud entre los miembros
  de un grupo) y la separación entre los diferentes grupos.

  El agrupamiento es un método de aprendizaje no supervisado y es una
  técnica muy popular de análisis estadístico de datos.

- *Redes bayesianas* Una red bayesiana, red de creencia o modelo
  acíclico dirigido es un modelo probabilístico que representa una
  serie de variables de azar y sus independencias condicionales a
  través de un grafo acíclico dirigido. Una red bayesiana puede
  representar, por ejemplo, las relaciones probabilísticas entre
  enfermedades y síntomas. Dados ciertos síntomas, la red puede usarse
  para calcular las probabilidades de que ciertas enfermedades estén
  presentes en un organismo. Hay algoritmos eficientes que infieren y
  aprenden usando este tipo de representación.


** Clasificador en Cascada

La detección de objetos usando un clasificador en cascada basado en
características de Haar es un método efectivo de detección de objetos
propuesto en 2001 por Paul Viola y Michael Jones en su artículo
"Detección rápida de objetos usando cascada mejorada de
características simples". Este es un método basado en el aprendizaje
automático en el que se entrena una función en cascada a partir de
muchas imágenes positivas y negativas. Luego se usa para detectar
objetos en otras imágenes.

El algoritmo requiere una gran cantidad de imágenes positivas
(imágenes faciales) e imágenes negativas (no imágenes faciales) para
entrenar al clasificador. Luego, necesitamos extraer características
de él. Para hacer esto, use la función Haar que se muestra en la
siguiente figura. Son como nuestros núcleos de convolución. Cada
característica es un valor único que se obtiene restando la suma de
píxeles debajo del rectángulo blanco de la suma de píxeles debajo del
rectángulo negro.

#+ATTR_ORG: :width 200
 [[file:img/har.png]]

Ahora, todos los tamaños y posiciones posibles de cada kernel se
utilizan para calcular muchas funciones. (Imagínese cuántos cálculos
genera. Incluso una ventana de 24x24 generará más de 160.000
características). Para el cálculo de cada característica, necesitamos
encontrar la suma de los píxeles debajo de los rectángulos blanco y
negro. Para resolver este problema, introdujeron la imagen general. No
importa qué tan grande sea su imagen, reducirá el cálculo de un píxel
dado a operaciones que involucren solo cuatro píxeles.

Pero de todas estas características que calculamos, la mayoría de
ellas no son relevantes. Por ejemplo, considere la siguiente
figura. La primera fila muestra dos buenas características. La primera
característica elegida pareció centrarse en la naturaleza del área de
los ojos, que generalmente es más oscura que las áreas de la nariz y
las mejillas. La segunda característica elegida se basa en que las
propiedades de los ojos son más oscuras que el puente de la nariz. Sin
embargo, aplicar la misma ventana en las mejillas o en cualquier otro
lugar es irrelevante. Entonces, ¿cómo elegimos la mejor función entre
más de 160.000 funciones?.

#+ATTR_ORG: :width 200
file:img/har2.png


Para ello, aplicamos todas las funciones a todas las imágenes de
entrenamiento. Para cada característica, encontrará el mejor umbral,
que divide el rostro en positivo y negativo. Obviamente, habrá errores
o clasificaciones erróneas. Elegimos las características con la tasa
de error más baja, lo que significa que son las características más
precisas para clasificar imágenes faciales y no faciales. (Este
proceso no es tan simple. Al principio, el peso de cada imagen es
igual. Después de cada clasificación, el peso de la imagen mal
clasificada aumentará. Luego se realizará el mismo proceso. Se
calculará la nueva tasa de error. También calcular Peso
nuevo. Continúe con este proceso hasta que se alcance la precisión o
la tasa de error requeridas o se encuentre el número requerido de
funciones.

El clasificador final es la suma ponderada de estos clasificadores
débiles. Se denomina clasificación débil porque no puede clasificar
imágenes por sí sola, sino que forma un clasificador fuerte junto con
otras clasificaciones. El documento dice que incluso 200 funciones
pueden proporcionar una detección de precisión del 95%. Su
configuración final tiene aproximadamente 6000 funciones. (Imagínese
reducir de más de 160.000 funciones a 6.000 funciones. Esto es una
gran ganancia).

Entonces ahora toma una foto. Toma cada ventana de 24x24. Aplicarle
6000 funciones. Busque caras. Vaya ... ¿No es esto ineficiente y
requiere mucho tiempo? Si. El autor tiene una buena solución para
esto.

En la imagen, la mayoría de las imágenes son áreas sin caras. Por lo
tanto, es mejor tener una manera fácil de verificar si la ventana no
es un área frontal. Si no es así, deséchelo todo de una vez y no lo
vuelva a procesar. En cambio, concéntrese en las áreas que pueden
tener caras. De esta forma, dedicaremos más tiempo a comprobar
posibles zonas faciales.

Con este fin, introdujeron el concepto de clasificadores en
cascada. En lugar de aplicar los 6000 componentes funcionales a una
ventana, estos componentes funcionales se agrupan en diferentes etapas
de clasificación y se aplican uno por uno. (Por lo general, las
primeras etapas contendrán muy pocas funciones). Si la ventana falla
en la primera etapa, se descarta. No consideramos sus funciones
restantes. Si pasa, se aplica la segunda etapa de la función y el
proceso continúa. La ventana a través de todas las etapas es un área
facial. ¡Qué tal este plan!

El detector del autor tiene más de 6000 características con 38 etapas,
con 1, 10, 25, 25 y 50 características en las primeras cinco
etapas. (Las dos funciones en la imagen de arriba son en realidad las
dos mejores funciones obtenidas de Adaboost). Según el autor, cada
subventana evaluó un promedio de 10 características de más de 6000
características.

Por lo tanto, esta es una explicación simple e intuitiva del principio
de funcionamiento de la detección de rostros Viola-Jones. Lea este
artículo para obtener más detalles o consulte las referencias en la
sección de otros recursos.

*** Ejemplo de clasificación utilizando Haarcascades 
	
	- [[https://github.com/opencv/opencv/tree/master/data/haarcascades][Clasificadores Haarcascades de la librería Opencv]]
	- [[https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html][Tutorial Haarcascades]]  
	- [[https://docs.opencv.org/2.4/doc/user_guide/ug_traincascade.html][Entrenamiento Haarcascades]]  


#+BEGIN_SRC python
import numpy as np
import cv2 as cv

rostro = cv.CascadeClassifier('haarcascade_frontalface_alt.xml')
cap = cv.VideoCapture(0)
x=y=w=h= 0 
img = 0
count = 0
while True:
    ret, frame = cap.read()
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    rostros = rostro.detectMultiScale(gray, 1.3, 5)
    for(x, y, w, h) in rostros:
        m= int(h/2)
        frame = cv.rectangle(frame, (x,y), (x+w, y+h), (0, 255, 0), 2)
        frame = cv.rectangle(frame, (x,y+m), (x+w, y+h), (255, 0 ,0), 2 )
        img = 180- frame[y:y+h,x:x+w]
        count = count + 1   
    
    #name = '/home/likcos/imgs/cara'+str(count)+'.jpg'
    #cv.imwrite(name, frame)
    cv.imshow('rostros', frame)
    cv.imshow('cara', img)
    
    k = cv.waitKey(1)
    if k == 27:
        break
cap.release()
cv.destroyAllWindows()
#+END_SRC

** Segmentación de Color  

La segmentación de imágenes es un tema ampliamente estudiado para la
extracción y reconocimiento de objetos, de acuerdo a las
características de textura, color, forma, entre otros. Dependiendo de
la naturaleza del problema, las características de color de los
objetos pueden proporcionar información relevante sobre ellos. Por
ejemplo, la segmentación de imágenes de color ha sido aplicado en
diferentes áreas como análisis de alimentos, geología,
medicina entre otras.  Los trabajos que abordan la
segmentación de imágenes por características de color emplean
diferentes técnicas, pero las más empleadas son las redes
neuronales (RN) y métodos basado en agrupamiento,
específicamente, fuzzy c-means (FCM). Las RN son entrenadas
para reconocer colores específicos, es decir, estas son entrenadas con
los colores de la imagen a ser segmentada. Si se da una nueva imagen
la RN debe ser entrenada nuevamente. Al emplear métodos basados en
agrupamiento, se crean grupos de colores con características
similares. La desventaja con tales métodos es que se requiere definir
previamente la cantidad de grupos en que se divide la información; por
lo tanto, el número de grupos se define dependiendo de la naturaleza
de la escena.  Nuestra propuesta consiste en entrenar a la RN para
reconocer diferentes colores, tratando de emular la percepción humana
del color. Los seres humanos identifican principalmente los colores
por su cromaticidad, después por su intensidad [21]. Por ejemplo, si
se le pregunta a cualquier persona cual es el color de los cuadros (a)
y (b) de la Fig. 1, lo más seguro es que responderá “verde”; nótese
que el cuadro (a) es más brilloso que el cuadro (b) pero la
cromaticidad no cambia. Ahora, si se le vuelve a preguntar a esa misma
persona cual es el color de los cuadros (c) y (d) de la Fig. 1, lo más
seguro es que responda “rojo y rosa, respectivamente”; es importante
mencionar que los cuadros (c) y (d) tienen la misma intensidad pero
diferentes cromaticidades.





#+BEGIN_SRC python
#+END_SRC




y
** Árboles de decisión 

 Los árboles de decisión (DT) son un método de aprendizaje supervisado
 no paramétrico que se utiliza para la clasificación y la regresión. El
 objetivo es crear un modelo que prediga el valor de una variable de
 destino mediante el aprendizaje de reglas de decisión simples
 deducidas de las características de los datos. Un árbol puede verse
 como una aproximación constante por partes.

 en el siguiente ejemplo, los árboles de decisión aprenden de los datos
 para aproximarse a una curva sinusoidal con un conjunto de reglas de
 decisión if-then-else. Cuanto más profundo es el árbol, más complejas
 son las reglas de decisión y más ajustado es el modelo

 #+ATTR_ORG: :width 500
 [[file:img/dtr.png]]


*** Elementos

 Los árboles de decisión están formados por nodos, vectores de números,
 flechas y etiquetas.

 - Cada nodo se puede definir como el momento en el que se ha de tomar
   una decisión de entre varias posibles, lo que va haciendo que a
   medida que aumenta el número de nodos aumente el número de posibles
   finales a los que puede llegar el individuo. Esto hace que un árbol
   con muchos nodos sea complicado de dibujar a mano y de analizar
   debido a la existencia de numerosos caminos que se pueden seguir.
 - Los vectores de números serían la solución final a la que se llega
   en función de las diversas posibilidades que se tienen, dan las
   utilidades en esa solución.
 - Las flechas son las uniones entre un nodo y otro y representan cada
   acción distinta.
 - Las etiquetas se encuentran en cada nodo y cada flecha y dan nombre
   a cada acción.

*** Algunas ventajas de los árboles de decisión son:

 - Fácil de entender y de interpretar. Los árboles se pueden
   visualizar.

 - Requiere poca preparación de datos. Otras técnicas a menudo
   requieren la normalización de datos, es necesario crear variables
   ficticias y eliminar valores en blanco. Sin embargo, tenga en cuenta
   que este módulo no admite valores faltantes.

 - El costo de usar el árbol (es decir, predecir datos) es logarítmico
   en la cantidad de puntos de datos usados ​​para entrenar el árbol.

- Capaz de manejar datos numéricos y categóricos. Otras técnicas
  suelen estar especializadas en analizar conjuntos de datos que
  tienen un solo tipo de variable.

- Capaz de manejar problemas de múltiples salidas.

- Utiliza un modelo de caja blanca. Si una situación dada es
  observable en un modelo, la explicación de la condición se explica
  fácilmente mediante lógica booleana. Por el contrario, en un modelo
  de caja negra (por ejemplo, en una red neuronal artificial), los
  resultados pueden ser más difíciles de interpretar.

- Posibilidad de validar un modelo mediante pruebas estadísticas. Eso
  permite dar cuenta de la fiabilidad del modelo.

- Tiene un buen desempeño incluso si sus supuestos son algo violados
  por el verdadero modelo a partir del cual se generaron los dato

*** Desventajas de los árboles de decisión:

- El aprendizaje de los  árboles de decisión pueden crear árboles demasiado
  complejos que no generalizan bien los datos. Esto se llama
  sobreajuste. Para evitar este problema, son necesarios mecanismos
  como la poda, establecer el número mínimo de muestras requeridas en
  un nudo de la hoja o establecer la profundidad máxima del árbol.

- Los árboles de decisión pueden ser inestables porque pequeñas
  variaciones en los datos pueden generar un árbol completamente
  diferente. Este problema se mitiga mediante el uso de árboles de
  decisión dentro de un conjunto.

- Las predicciones de los árboles de decisión no son uniformes ni
  continuas, sino aproximaciones constantes por partes, como se ve en
  la figura anterior. Por lo tanto, no son buenos para la
  extrapolación.

- Se sabe que el problema de aprender un árbol de decisión óptimo es
  NP-completo bajo varios aspectos de optimización e incluso para
  conceptos simples. En consecuencia, los algoritmos prácticos de
  aprendizaje del árbol de decisiones se basan en algoritmos
  heurísticos, como el algoritmo voraz, en el que se toman decisiones
  localmente óptimas en cada nodo. Dichos algoritmos no pueden
  garantizar la devolución del árbol de decisión globalmente
  óptimo. Esto se puede mitigar entrenando varios árboles en un alumno
  de conjunto, donde las características y las muestras se muestrean
  aleatoriamente con reemplazo.

- Hay conceptos que son difíciles de aprender porque los árboles de
  decisión no los expresan fácilmente, como XOR, paridad o problemas
  de multiplexor.

- Los aprendices de árboles de decisión crean árboles sesgados si
  dominan algunas clases. Por lo tanto, se recomienda equilibrar el
  conjunto de datos antes de ajustarlo al árbol de decisión.

*** Ejemplo de Clasificación, Árbol de decisión scikit-learn 

*DecisionTreeClassifier* es una clase capaz de realizar una
clasificación de varias clases en un conjunto de datos.

Al igual que con otros clasificadores, *DecisionTreeClassifier* toma
como entrada dos matrices: una matriz *X*, dispersa o densa, de forma
(n_muestras, n_características) que contiene las muestras de
entrenamiento, y una matriz *Y* de valores enteros, forma (n_muestras),
que contiene las etiquetas de clase. para las muestras de
entrenamiento:

#+BEGIN_SRC python
from sklearn import tree
X = [[0, 0], [1, 1]]
Y = [0, 1]
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)
clf.predict([[2.,2.]]) 
#+END_SRC


Despues de ajustarce el modelo se puede usar, para predecir la clase 
#+BEGIN_SRC python 
clf.predict([[2.,2.]]) 
#array([1])
#+END_SRC


En caso de que haya múltiples clases con la misma y mayor
probabilidad, el clasificador predecirá la clase con el índice más
bajo entre esas clases.

Como alternativa a generar una clase específica, se puede predecir la
probabilidad de cada clase, que es la fracción de muestras de
entrenamiento de la clase en una hoja:

#+BEGIN_SRC python
clf.predict_proba([[2., 2.]])
#array([[0., 1.]])
#+END_SRC


DecisionTreeClassifier es capaz tanto de clasificación binaria (donde
las etiquetas son [-1, 1]) como de clasificación multiclase (donde las
etiquetas son [0, …, K-1]).

Usando el conjunto de datos de Iris, podemos construir un árbol de la
siguiente manera:

#+BEGIN_SRC python :results output :tangle code/arbol.py
# Importar las bibliotecas necesarias
from sklearn.datasets import load_iris
from sklearn import tree
import graphviz

# Cargar el conjunto de datos Iris
iris = load_iris()

X, y = iris.data, iris.target
print( X, y)
# Crear el clasificador del Árbol de Decisión
clf = tree.DecisionTreeClassifier()

# Entrenar el modelo con los datos
clf = clf.fit(X, y)

# Exportar el árbol de decisión en formato DOT para su visualización
dot_data = tree.export_graphviz(clf, out_file=None, 
                                feature_names=iris.feature_names,  
                                class_names=iris.target_names,  
                                filled=True, rounded=True,  
                                special_characters=True)  

# Crear el gráfico con graphviz
graph = graphviz.Source(dot_data)

# Guardar el gráfico como un archivo PDF (opcional)
graph.render("iris_decision_tree")

# Mostrar el gráfico directamente
graph.view()

#+END_SRC

#+RESULTS:

*** Árbol de Decisión en el Conjunto de Datos Iris

**** Estructura del Árbol de Decisión
Un árbol de decisión tiene la siguiente estructura:
- **Raíz del árbol**: El nodo inicial donde comienza la primera división.
- **Nodos internos**: Nodos que dividen los datos en función de una característica.
- **Hojas**: Nodos finales donde se realiza la clasificación. No hay más divisiones posibles en estos nodos.

**** Nodo raíz
La primera decisión se toma en base a **petal width (cm) ≤ 0.8**:
- **gini = 0.667**: El índice de Gini mide la impureza del nodo. Un
  valor de 0 significa pureza total (todas las muestras pertenecen a
  una clase), mientras que un valor de 0.667 indica mezcla.
- **samples = 150**: Hay 150 muestras en este nodo.
- **value = [50, 50, 50]**: Hay 50 flores de cada clase (**setosa**, **versicolor**, **virginica**).
- **class = setosa**: Aunque el nodo está mezclado, la clase mayoritaria es **setosa**.

**** Primera división
- **Si petal width (cm) ≤ 0.8**:
  - **gini = 0.0**, **samples = 50**, **value = [50, 0, 0]**. Todas las flores en este nodo son de la clase **setosa**, por lo que el modelo puede clasificarlas sin más divisiones.
  
- **Si petal width (cm) > 0.8**:
  - El modelo sigue dividiendo los datos de las otras dos clases (**versicolor** y **virginica**).

**** Segunda división
Para el resto de las flores, el modelo utiliza la característica **petal width (cm) ≤ 1.75**:
- **gini = 0.5**, **samples = 100**, **value = [0, 50, 50]**. Aquí hay una mezcla equilibrada de las clases **versicolor** y **virginica**.
- Si el ancho del pétalo es menor o igual a 1.75 cm, el modelo predice **versicolor**, mientras que si es mayor, predice **virginica**.

**** Divisiones más profundas
El árbol sigue dividiendo basándose en otras características, como **petal length (cm) ≤ 4.95**:
- Si esta condición se cumple, el modelo predice **versicolor**. Si no, comienza a clasificar flores como **virginica**.

**** Hojas finales
Cada nodo hoja finaliza la clasificación:
- Si un nodo tiene **gini = 0.0**, significa que todas las muestras en ese nodo pertenecen a una sola clase.
- Por ejemplo, en un nodo final con **gini = 0.0**, **samples = 43**, **value = [0, 0, 43]**, todas las flores en ese nodo son de la clase **virginica**.

*** Índice de Gini en Árboles de Decisión

**** ¿Qué es el índice de Gini?
El **índice de Gini** es una medida de impureza o diversidad que se
utiliza en algoritmos de aprendizaje automático, como los Árboles de
Decisión. Se utiliza para medir qué tan puras o mezcladas están las
clases en un nodo del árbol.

**** Concepto del Índice de Gini
El índice de Gini mide la probabilidad de que una muestra elegida al
azar sea clasificada incorrectamente si se la asigna una clase de
manera aleatoria basada en la distribución de clases en ese nodo.

- **Un nodo puro** tendrá un índice de Gini igual a 0, lo que significa
  que todas las muestras en ese nodo pertenecen a la misma clase.
- **Un nodo impuro** tendrá un índice de Gini mayor, lo que indica que
  hay una mezcla de clases en ese nodo.

**** Fórmula del Índice de Gini
El índice de Gini se calcula como:

\[
Gini = 1 - \sum_{i=1}^{n} p_i^2
\]

Donde:
- **n** es el número de clases.
- **p_i** es la proporción de ejemplos de la clase **i** en el nodo.

**** Ejemplos del Índice de Gini

***** Ejemplo 1: Nodo completamente puro
Si todas las muestras en un nodo pertenecen a una sola clase, entonces el índice de Gini es **0**.

- Ejemplo: Si hay 100 muestras y todas pertenecen a la clase "A", entonces \( p_A = 1 \) y las demás probabilidades son 0.
- Cálculo:
#+BEGIN_SRC python
Gini = 1 - (1^2) = 0
#+END_SRC
Esto indica que el nodo es completamente puro.

***** Ejemplo 2: Nodo con clases mixtas
Si las muestras están distribuidas equitativamente entre varias clases, el índice de Gini será mayor.

- Ejemplo: Si hay 100 muestras, con 50 de la clase "A" y 50 de la clase "B", entonces \( p_A = 0.5 \) y \( p_B = 0.5 \).
- Cálculo:
#+BEGIN_SRC python
Gini = 1 - (0.5^2 + 0.5^2) = 1 - 0.25 - 0.25 = 0.5
#+END_SRC
Esto indica que el nodo contiene una mezcla de clases.

***** Ejemplo 3: Nodo con clases desbalanceadas
Si en un nodo hay 90 muestras de la clase "A" y 10 de la clase "B", el índice de Gini será más bajo que en el caso de clases equilibradas.

- Ejemplo: \( p_A = 0.9 \) y \( p_B = 0.1 \).
- Cálculo:
#+BEGIN_SRC python
Gini = 1 - (0.9^2 + 0.1^2) = 1 - 0.81 - 0.01 = 0.18
#+END_SRC
Esto indica que la mayoría de las muestras pertenecen a una clase, pero el nodo aún no es completamente puro.

**** Interpretación del Índice de Gini
- **Gini = 0**: El nodo es completamente puro (todas las muestras pertenecen a una sola clase).
- **Gini cercano a 0**: La mayoría de las muestras pertenecen a una clase, pero hay algo de mezcla.
- **Gini cercano a 1**: El nodo es muy impuro, con una mezcla casi uniforme de varias clases.

**** Uso en Árboles de Decisión
El árbol de decisión selecciona la característica que minimiza el índice de Gini en los nodos hijos, creando divisiones que generen nodos lo más puros posibles.

**** Ejemplo del Índice de Gini en tu Árbol de Decisión
En el nodo raíz del árbol de decisión que generaste, el **índice de Gini** es 0.667:
- Esto indica que hay una mezcla significativa de clases en ese nodo.
- El árbol de decisión divide los datos para reducir el índice de Gini en los nodos hijos.

*** Ejemplo con Dataset Phaser

#+begin_src python :results output :tangle code/arbolphaser.py
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, export_graphviz
import graphviz

# Cargar el dataset
file_path = 'phaser.csv'
dataset = pd.read_csv(file_path)

# Eliminar columnas innecesarias (como la vacía "Unnamed: 3")
#dataset = dataset.drop(columns=['Unnamed: 3'])

# Definir características (X) y etiquetas (y)
X = dataset.iloc[:, :2]  # Las dos primeras columnas son las características
y = dataset.iloc[:, 2]   # La tercera columna es la etiqueta

print(X)
# Dividir los datos en conjunto de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Crear el clasificador de Árbol de Decisión
clf = DecisionTreeClassifier()

# Entrenar el modelo
clf.fit(X_train, y_train)


y_predict = clf.predict(X_test)

print(X_test, y_predict)
# Exportar el árbol de decisión en formato DOT para su visualización
dot_data = export_graphviz(clf, out_file=None, 
                           feature_names=['Feature 1', 'Feature 2'],  
                           class_names=['Clase 0', 'Clase 1'],  
                           filled=True, rounded=True,  
                           special_characters=True)  

# Crear el gráfico con graphviz
graph = graphviz.Source(dot_data)

# Mostrar el gráfico
graph.view()

#+end_src

#+RESULTS:
#+begin_example
     650  548
0    632  548
1    614  548
2    596  548
3    577  548
4    559  548
..   ...  ...
234  570  302
235  560  302
236  550  302
237  540  302
238  530  302

[239 rows x 2 columns]
     650  548
24   194  548
6    523  548
93    53  332
109  446  472
104  525  472
172   49  531
200  324  466
86   130  332
9    468  548
142  580  531
45   584  332
205  247  466
114  367  472
167  137  531
113  383  472
229  620  302
184  573  466
15   358  548
220   14  466
125  194  472
127  163  472
19   285  548
234  570  302
30    84  548
175    4  531
218   45  466
10   450  548
186  542  466
236  550  302
152  403  531
96   650  472
213  122  466
25   176  548
181  619  466
18   303  548
69   318  332
211  153  466
55   473  332
79   208  332
196  386  466
226  650  302
112  399  472
150  438  531
16   340  548
155  350  531
66   352  332
164  190  531
38    50  548 [1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 1
 0 1 0 1 0 0 0 0 0 0 1]
#+end_example


** Arquitecturas de Redes Neuronales

*** 1. Redes Neuronales Feedforward (FFNN)
   - Las **redes neuronales feedforward** son las más simples y
     consisten en capas donde la información fluye en una sola
     dirección, desde la capa de entrada hasta la capa de salida, sin
     retroalimentación.
   - La arquitectura de un perceptrón multicapa (MLP) es un ejemplo de
     red feedforward.
   
   *** Aplicaciones comunes:
       - Clasificación de datos estructurados (tabulares).
       - Problemas de regresión.


*** 1.1 Ejemplo de Redes Neuronales Feedforward

****  fit_transform y transform

Ambos son los métodos de la clase
sklearn.preprocessing.StandardScaler() y se utilizan casi juntos
mientras escalan o estandarizan nuestros datos de entrenamiento y
prueba.


- La estandarización de los datos es el proceso de reescalar los
  atributos para que tengan una media de 0 y una varianza de 1.

- El objetivo final de realizar la estandarización es reducir todas
  las características a una escala común sin distorsionar las
  diferencias en el rango de los valores.

- En sklearn.preprocessing.StandardScaler(), el centrado y el escalado
  ocurren independientemente en cada característica.


**** fit_transform()

fit_transform() se utiliza en los datos de entrenamiento para que
podamos escalar los datos de entrenamiento y también aprender los
parámetros de escalado de esos datos. Aquí, el modelo construido por
nosotros aprenderá la media y la varianza de las características del
conjunto de entrenamiento. Estos parámetros aprendidos se utilizan
para escalar nuestros datos de prueba.



**** transform()

Utilizando el método transform() podemos utilizar la misma media y
varianza que se calcula a partir de nuestros datos de entrenamiento
para transformar nuestros datos de prueba. Así, los parámetros
aprendidos por nuestro modelo utilizando los datos de entrenamiento
nos ayudarán a transformar nuestros datos de prueba.


#+BEGIN_SRC python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

# Cargar el dataset Iris
iris = load_iris()
X, y = iris.data, iris.target

# Dividir en conjunto de entrenamiento y prueba (80%-20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Normalizar las características (importante para MLP)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Definir la red neuronal con una capa oculta de 10 neuronas
mlp = MLPClassifier(hidden_layer_sizes=(10,), activation='relu', solver='adam', max_iter=500, random_state=42)
# Entrenar el modelo
mlp.fit(X_train, y_train)
# Hacer predicciones
y_pred = mlp.predict(X_test)
# Evaluar el modelo
accuracy = accuracy_score(y_test, y_pred)
print(f'\nPrecisión en test: {accuracy:.4f}')
#+END_SRC

**** Joblib

joblib es una librería de Python utilizada para guardar y cargar
modelos de machine learning de manera eficiente. Se usa mucho en
scikit-learn porque maneja mejor estructuras grandes como arrays de
NumPy y modelos entrenados que pickle.

¿Para qué sirve joblib?

- Guardar modelos de machine learning para reutilizarlos sin volver a entrenar.
- Cargar modelos previamente entrenados para hacer predicciones en nuevos datos.
- Optimizar el almacenamiento de datos grandes, ya que usa compresión eficiente.


¿Cuál es la diferencia entre joblib y pickle?

| Característica            | joblib      | pickle         |
|---------------------------+-------------+----------------|
| Optimizado para NumPy     | Sí          | No             |
|---------------------------+-------------+----------------|
| Manejo de modelos grandes | Sí          | No             |
|---------------------------+-------------+----------------|
| Uso en scikit-learn       | Recomendado | No recomendado |
|---------------------------+-------------+----------------|


#+BEGIN_SRC python
import joblib
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

# Cargar el dataset Iris
iris = load_iris()
X, y = iris.data, iris.target

# Dividir en conjunto de entrenamiento y prueba (80%-20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalizar las características (importante para MLP)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Definir la red neuronal con una capa oculta de 10 neuronas
mlp = MLPClassifier(hidden_layer_sizes=(10,), activation='relu', solver='adam', max_iter=500, random_state=42)

# Entrenar el modelo
mlp.fit(X_train, y_train)

# Hacer predicciones
y_pred = mlp.predict(X_test)

# Evaluar el modelo
accuracy = accuracy_score(y_test, y_pred)
print(f'\nPrecisión en test: {accuracy:.4f}')

# Guardar el modelo y el scaler para su uso posterior
joblib.dump(mlp, 'mlp_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
print("\nModelo y scaler guardados correctamente.")

#+END_SRC


- Cargar modelo y meter un dato nuevo



#+BEGIN_SRC python :results output
import joblib
import numpy as np

# Cargar el modelo y el scaler
mlp_loaded = joblib.load('/home/likcos/mlp_model.pkl')
scaler_loaded = joblib.load('/home/likcos/scaler.pkl')

# Nuevo dato (4 características como en el dataset Iris)
nuevo_dato = np.array([[0, 0, 0, 0]])

# Normalizar el nuevo dato
nuevo_dato_escalado = scaler_loaded.transform(nuevo_dato)

# Hacer la predicción
prediccion = mlp_loaded.predict(nuevo_dato_escalado)

# Diccionario para interpretar la clase
clases = {0: 'Setosa', 1: 'Versicolor', 2: 'Virginica'}

print(f'La flor pertenece a la clase: {clases[prediccion[0]]}')
#+END_SRC

#+RESULTS:
: La flor pertenece a la clase: Versicolor


**** ¿Qué es Softmax?
La función *softmax* es una función de activación utilizada en la capa de salida de redes neuronales para clasificación multiclase. Convierte un conjunto de valores en una distribución de *probabilidades*.

**** Fórmula:
\[
\sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
\]
Donde:
- \( z_i \) es la activación de la neurona \( i \).
- \( e^{z_i} \) es la exponencial de la activación.
- \( \sum e^{z_j} \) es la suma de todas las exponenciales de las activaciones.

**** Ejemplo práctico:
Si la red neuronal genera los siguientes valores antes de aplicar *softmax* (logits):
#+BEGIN_SRC python
z = [2.0, 1.0, 0.1]
#+END_SRC

Aplicando la función softmax:

\[
\sigma(2.0) = \frac{e^2}{e^2 + e^1 + e^{0.1}} = 0.659
\]

\[
\sigma(1.0) = \frac{e^1}{e^2 + e^1 + e^{0.1}} = 0.242
\]

\[
\sigma(0.1) = \frac{e^{0.1}}{e^2 + e^1 + e^{0.1}} = 0.099
\]

Salida final:

#+BEGIN_SRC python
[0.659, 0.242, 0.099]
#+END_SRC

Esto significa que la red asigna una probabilidad del *65.9%* a la primera clase, *24.2%* a la segunda y *9.9%* a la tercera.

-  *Convierte las salidas en probabilidades* (valores entre 0 y 1 que suman 1).
-  *Permite interpretar las predicciones* fácilmente.
-  *Destaca la clase más probable* con valores amplificados.

Para clasificación binaria se usa la función *sigmoide* en vez de *softmax*.
En *scikit-learn*, la función *softmax* ya está integrada en *MLPClassifier* para clasificación multiclase.




**** Descenso del Gradiente 
El descenso del gradiente o gradiente descendiente es un algoritmo de
optimización iterativo de primer orden que permite encontrar mínimos
locales en una función diferenciable. La idea es tomar pasos de manera
repetida en dirección contraria al gradiente. Esto se hace ya que esta
dirección es la del descenso más empinado. Si se toman pasos con la
misma dirección del gradiente, se encontrará el máximo local de la
función; a esto se le conoce como el gradiente ascendente. Este
algoritmo es utilizado para entrenar modelos de aprendizaje máquina y
redes neuronales.

**** Momentum

Stochastic Gradient Descent with Momentum o, simplemente, Momentum, es
una variación del algoritmo de descenso de gradiente estocástico (SGD)
que utiliza una técnica de suavizado para evitar oscilaciones
excesivas en la dirección del gradiente y aumentar la velocidad de
convergencia: En lugar de actualizar los parámetros utilizando solo la
información del gradiente actual (tras evaluar n muestras), el
algoritmo de Momentum utiliza un promedio ponderado del gradiente
actual y de los gradientes anteriores para calcular la dirección de
actualización (es decir, el gradiente a considerar para la
actualización de los parámetros).

Este algoritmo requiere dos parámetros: el coeficiente de aprendizaje
que ya conocemos (learning rate) y el llamado coeficiente de
momentum. El coeficiente de momentum controla la influencia de los
gradientes anteriores en la dirección de actualización. En función de
los datos, un coeficiente de momentum alto puede ayudar a superar los
mínimos locales en la función de error y llevar a una convergencia más
rápida.

La actualización de los pesos en el algoritmo de Momentum se realiza,
por lo tanto, de la siguiente manera:

- Se evalúa el gradiente actual tras considerar n muestras
- Se calcula la dirección de actualización como una combinación del gradiente actual y los gradientes anteriores ponderados usando el coeficiente de momentum
- Se actualizan los parámetros del modelo utilizando la dirección de actualización y el coeficiente de aprendizaje
- El algoritmo de Momentum se puede implementar de diferentes maneras, como utilizando un promedio ponderado simple de los gradientes anteriores o utilizando un promedio ponderado exponencial.

**** Adam

Adam, o Adaptive Moment Estimation, es un algoritmo de optimización
que combina las ventajas de los algoritmos RMSprop y Momentum para
mejorar el proceso de aprendizaje de un modelo. Al igual que Momentum,
Adam utiliza una estimación del momento y de la magnitud de los
gradientes anteriores para actualizar los parámetros del modelo en
cada iteración. Sin embargo, en lugar de utilizar una tasa de
aprendizaje constante para todos los parámetros, Adam adapta la tasa
de aprendizaje de cada parámetro individualmente en función de su
estimación del momento y de la magnitud del gradiente. Esto permite
que el modelo se ajuste de manera más eficiente y efectiva a los datos
de entrenamiento, lo que puede llevar a una mayor precisión de la
predicción en comparación con otros métodos de optimización.


	   

	   
*** 2. Redes Neuronales Convolucionales (CNN)
   - Las **redes neuronales convolucionales** están diseñadas para
     procesar datos que tienen una estructura de cuadrícula, como
     imágenes. Las CNN usan convoluciones para reducir la cantidad de
     parámetros y hacen que las redes sean eficientes para el
     procesamiento de imágenes.

   *** Componentes principales:
       - **Capas de Convolución**: Detectan características en los
         datos, como bordes y texturas.
       - **Capas de Pooling**: Reducen la dimensionalidad y ayudan a
         evitar el sobreajuste.
       - **Capas de Conexión Completa**: Al final de la red, para
         clasificar o predecir.

   *** Aplicaciones comunes:
       - Clasificación de imágenes.
       - Detección y segmentación de objetos.
       - Procesamiento de video.

   *** Ejemplo en Python:
       #+begin_src python
       import tensorflow as tf
       from tensorflow.keras import layers, models

       model = models.Sequential([
           layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),
           layers.MaxPooling2D((2, 2)),
           layers.Conv2D(64, (3, 3), activation='relu'),
           layers.MaxPooling2D((2, 2)),
           layers.Flatten(),
           layers.Dense(64, activation='relu'),
           layers.Dense(10, activation='softmax')
       ])
       #+end_src

*** 3. Redes Neuronales Recurrentes (RNN)
   - Las **redes neuronales recurrentes** están diseñadas para trabajar
     con datos secuenciales o series temporales. Las RNN tienen una
     conexión recurrente que les permite recordar información de
     entradas anteriores.
   
   *** Variantes de RNN:
       - **LSTM (Long Short-Term Memory)**: Diseñadas para retener
         información en secuencias largas y resolver el problema del
         gradiente desvaneciente.
       - **GRU (Gated Recurrent Unit)**: Variante simplificada de LSTM que es más eficiente computacionalmente.

   *** Aplicaciones comunes:
       - Procesamiento de lenguaje natural (NLP).
       - Predicción de series temporales.
       - Generación de texto.

   *** Ejemplo en Python:
       #+begin_src python
       from tensorflow.keras.layers import SimpleRNN, LSTM, Dense
       from tensorflow.keras.models import Sequential

       model = Sequential([
           LSTM(50, input_shape=(100, 1)),
           Dense(1)
       ])
       #+end_src

*** 4. Redes Generativas Antagónicas (GAN)
   - Las **redes generativas antagónicas** (GAN) consisten en dos redes:
     una red generadora y una red discriminadora que compiten entre
     sí. La red generadora intenta crear datos similares a los datos
     reales, mientras que la red discriminadora trata de distinguir
     entre datos reales y generados.

   *** Componentes principales:
       - **Generador**: Produce datos falsos similares a los datos de entrenamiento.
       - **Discriminador**: Evalúa la autenticidad de los datos.
       - **Entrenamiento**: Ambos modelos se entrenan de forma simultánea en un proceso antagónico.

   *** Aplicaciones comunes:
       - Generación de imágenes realistas.
       - Aumento de datos (data augmentation).
       - Transferencia de estilo.

   *** Ejemplo en Python:
       #+begin_src python
       from tensorflow.keras.layers import Dense, LeakyReLU
       from tensorflow.keras.models import Sequential

       # Modelo Generador
       generator = Sequential([
           Dense(256, input_dim=100),
           LeakyReLU(0.2),
           Dense(512),
           LeakyReLU(0.2),
           Dense(784, activation='tanh')
       ])

       # Modelo Discriminador
       discriminator = Sequential([
           Dense(512, input_shape=(784,)),
           LeakyReLU(0.2),
           Dense(256),
           LeakyReLU(0.2),
           Dense(1, activation='sigmoid')
       ])
       #+end_src

*** 5. Redes de Memoria a Largo y Corto Plazo (LSTM)
   - Las **LSTM** son una variante de las RNN que utilizan un sistema de puertas (input gate, forget gate y output gate) para controlar el flujo de información y decidir qué recordar y qué olvidar.

   *** Aplicaciones comunes:
       - Traducción automática.
       - Generación de texto.
       - Análisis de sentimientos.

*** 6. Redes de Transformadores
   - Los **transformadores** utilizan un mecanismo de auto-atención para
     procesar datos secuenciales en paralelo. Esta arquitectura ha
     revolucionado el procesamiento del lenguaje natural.

   *** Componentes principales:
       - **Auto-atención**: Permite a la red enfocarse en diferentes
         partes de la entrada sin procesarlas en orden secuencial.
       - **Embeddings**: Representaciones vectoriales de las palabras.
       - **Capas de Feedforward y Atención Multi-Cabeza**: Proveen
         capacidad de aprendizaje al procesar la información.

   *** Aplicaciones comunes:
       - Traducción automática.
       - Resumen de texto.
       - Generación de lenguaje (GPT-3, BERT).

   *** Ejemplo en Python:
       #+begin_src python
       from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

       tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
       model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")

       # Tokenización de entrada
       inputs = tokenizer("Hello, how are you?", return_tensors="tf")

       # Predicción
       outputs = model(inputs)
       #+end_src

*** 7. Redes Autoencoders
   - Los **autoencoders** son redes neuronales diseñadas para aprender representaciones de datos de alta dimensionalidad en un espacio de menor dimensión. Consisten en dos partes principales: el codificador y el decodificador.

   *** Componentes principales:
       - **Codificador (Encoder)**: Reduce la dimensión del dato de entrada.
       - **Decodificador (Decoder)**: Reconstruye el dato original a partir de la representación reducida.

   *** Aplicaciones comunes:
       - Compresión de imágenes.
       - Eliminación de ruido en datos.
       - Detección de fraudes.

   *** Ejemplo en Python:
       #+begin_src python
       from tensorflow.keras.layers import Input, Dense
       from tensorflow.keras.models import Model

       # Autoencoder simple
       input_img = Input(shape=(784,))
       encoded = Dense(32, activation='relu')(input_img)
       decoded = Dense(784, activation='sigmoid')(encoded)

       autoencoder = Model(input_img, decoded)
       #+end_src

*** 8. Redes de Cápsulas (Capsule Networks)
   - Las **Capsule Networks** fueron propuestas para superar las limitaciones de las CNN en la detección de la rotación y posición espacial de los objetos en imágenes. Estas redes usan cápsulas (grupos de neuronas) para capturar mejor la orientación y jerarquía en las características de la imagen.

   *** Aplicaciones comunes:
       - Clasificación de imágenes donde la orientación y posición de los objetos son relevantes.

	   
* Análisis de información  
** Programación 
*** Carga de imágenes y propiedades  

Con el siguiente ejemplo, se puede cargar una imagen, utilizando la
librería de opencv, mediante el método [[https://docs.opencv.org/3.4/d4/da8/group__imgcodecs.html][imread]], el cual carga en la
variable **img** la imagen contenida en el directorio que se puede
observar en la linea 2 de igual forma el método [[https://www.geeksforgeeks.org/python-opencv-cv2-imshow-method/][imshow]] permite mostrar
la imagen, dando como primer parámetro, el nombre del marco y
posteriormente el nombre de la imagen, finalmente se utiliza el método
[[https://www.geeksforgeeks.org/python-opencv-waitkey-function/][waitKey]] permite  mostrar una ventana durante un número
específico de milisegundos o hasta que se presione cualquier tecla, a su vez 
la el método [[https://www.geeksforgeeks.org/python-opencv-destroyallwindows-function/][destroyAllWindows]] permite destruir todas las ventanas abiertas 
creadas por **imshow**. 



#+BEGIN_SRC python :results output
import cv2 as cv 
img = cv.imread("/home/likcos/Imágenes/tr.png")
cv.imshow('marco', img)
cv.waitKey()
cv.destroyAllWindows()
#+END_SRC

#+RESULTS:

Opencv también permite acceder a las propiedades de la imagen mediante
la función [[https://docs.opencv.org/3.4/d3/df2/tutorial_py_basic_ops.html][shape]], con la cual podremos acceder al tamaño de la imagen,
los canales de color entre otras propiedades. En el ejemplo siguiente
se muestra como al cargar la imagen utilizando la bandera de **1**
podemos acceder al ancho, alto y al numero de canales de color de la
imagen. En caso de cargar la imagen utilizando el **0**, **shape**
solo podrá acceder al alto, ancho de la imagen.


#+BEGIN_SRC python :results output
import cv2 as cv 
img = cv.imread("/home/likcos/Imágenes/tr.png",1)
w,h,c = img.shape
print(w,h,c)
cv.imshow('marco', img)
cv.waitKey()
cv.destroyAllWindows()

#+END_SRC

#+RESULTS:
: 632 635 3
*** Operadores Puntuales

De igual forma cargando la imagen, en un solo canal es posible aplicar
operadores puntuales a la imagen. Las operaciones puntuales son
transformaciones de uno a uno, es decir el nuevo valor de un pixel 'q'
en la posición ( i , j ) esta en función de un pixel 'p' de otra
imagen pero en la misma posición, es decir, ( i , j ).

#+BEGIN_SRC python :results output
import cv2 as cv
import numpy as np

img = cv.imread('tr.png', 0)
print(img.shape, img.size)
w = img.shape[0]
h = img.shape[1]
#c = img.shape[2]
#w1, h1 = img.shape
img2 = img
cv.imshow('imagen', img2)
for x in range(w):
    for y in range(h):
        if(img[x,y]>50):
            img[x,y]=255
        else:
            img[x,y]=0

cv.imshow('imagen1', img)
cv.waitKey(0)
cv.destroyAllWindows()
#+END_SRC

*** Modelos de color 

El color es una de las características que nos permite a los seres
humanos identificar y clasificar los objetos. La percepción visual del
color se genera en nuestro cerebro a partir de las ondas
electromagnéticas reflejadas por los objetos y captadas por los ojos.
Desde el punto de vista del procesamiento de imágenes por computador,
es preciso recurrir a los llamados espacios de color, se trata de
conjuntos de fórmulas matemáticas que permiten describir los colores y
descomponerlos en distintos canales. Los espacios de color más
utilizados son el RGB y el CMYK, debido a que el modelo RGB se utiliza
en periféricos como pantallas, cámaras y escáneres, y el modelo CMYK
en impresoras. En este capítulo se analizará también el sistema de
coordenadas tridimensional (tono, saturación e intensidad) del espacio
de color HSI, donde cada color está representado por un punto en el
espacio. Los dos espacios siguientes que se estudiarán fueron
establecidos por la Comisión Internacional de Iluminación (CIE): el
espacio de color XYZ se usa actualmente como referencia para definir
los colores que percibe el ojo humano, mientras que el Lab se puede
considerar como el más completo de los desarrollados por la CIE, ya
que permite identificar cada color de una forma muy precisa mediante
sus valores “a” y “b” y su brillo (“L”).  Finalmente, se analizará el
espacio de color YCbCr.

#+BEGIN_SRC python
img = cv.imread('tr.png', 1)
img2 = cv.cvtColor(img, cv.COLOR_BGR2RGB)
cv.imshow('img', img)
cv.imshow('img2', img2)

zero = np.zeros(img.shape[:2], dtype='uint8')
(r,g,b)= cv.split(img)
#cv.imshow('rg', r)
#cv.imshow('gg', g)
#cv.imshow('bg', b)
cv.imshow('GRB', cv.merge([g,r,b]))
#cv.imshow('G', cv.merge([zero,g,zero]))
#cv.imshow('R', cv.merge([zero,zero,r]))
#(r1,g1,b1)= cv.split(img2)
#cv.imshow('R1', cv.merge([r1,zero,zero]))
#cv.imshow('G1', cv.merge([zero,g1,zero]))
#cv.imshow('B1', cv.merge([zero,zero,b1]))

cv.waitKey(0)
cv.destroyAllWindows()
#+END_SRC

*** Segmentación de Color 

#+BEGIN_SRC python
import cv2 as cv
img = cv.imread('salida.jpg', 1)
img2 = cv.cvtColor(img, cv.COLOR_BGR2RGB)
img3 = cv.cvtColor(img2, cv.COLOR_RGB2HSV)

umbralBajo=(0, 80, 80  )
umbralAlto=(10, 255, 255)
umbralBajoB=(170, 80,80)
umbralAltoB=(180, 255, 255)


mascara1 = cv.inRange(img3, umbralBajo, umbralAlto)
mascara2 = cv.inRange(img3, umbralBajoB, umbralAltoB)

mascara = mascara1 + mascara2

resultado = cv.bitwise_and(img, img, mask=mascara)

cv.imshow('resultado', resultado)
cv.imshow('mascara', mascara)
cv.imshow('img',img)
cv.imshow('img2', img2)
cv.imshow('img3', img3)

cv.waitKey(0)
cv.destroyAllWindows()
#+END_SRC

#+RESULTS:
: None


*** Haarcascades 
Los Haar Cascades son una técnica utilizada en el campo de la visión
por computadora para la detección de objetos. Fueron introducidos por
Paul Viola y Michael Jones en su artículo seminal "Rapid Object
Detection using a Boosted Cascade of Simple Features" en 2001. Esta
técnica es particularmente conocida por su eficacia en la detección de
rostros, aunque puede ser utilizada para detectar otros tipos de
objetos.

#+startup: inlineimages
#+ATTR_LATEX: :width 0.3\textwidth
[[file:img/cascade.png]]

**** Conceptos Clave: 
Características de Haar: Son patrones visuales simples que se pueden
 calcular rápidamente en una imagen. Estas características se asemejan
 a pequeñas versiones de núcleos de wavelet de Haar y son utilizadas
 para capturar la presencia de bordes, cambios de textura, y otras
 propiedades visuales.

 
**** Imágenes Integrales: 
Para acelerar el cálculo de las características
 de Haar, se utiliza un concepto llamado imagen integral. Una imagen
 integral permite calcular la suma de los valores de los píxeles en
 cualquier área rectangular de la imagen en tiempo constante.

****  Adaboost: 
Es un método de aprendizaje automático utilizado para
 mejorar la eficiencia de la detección. Selecciona un pequeño número
 de características críticas de un conjunto más grande y construye
 clasificadores "débiles". Luego, estos se combinan en un clasificador
 más fuerte y eficiente.

****  Cascadas: 
En lugar de aplicar todas las características a una ventana de la
imagen, se organizan en una secuencia de etapas (cascadas). Cada etapa
tiene su propio clasificador (hecho con Adaboost) y solo pasa las
ventanas de la imagen que parecen prometedoras. Esto reduce
significativamente el tiempo de cálculo, ya que muchas ventanas no
pasan las primeras etapas.

 *Proceso de Detección*: 
Pre-procesamiento: Se convierte la imagen en
 escala de grises y se crea su imagen integral.

 *Aplicación de las Características*: Se desplaza una ventana sobre la
 imagen, y en cada posición, se calculan las características de Haar.

 *Clasificación en Cascada*: Cada ventana es evaluada a través de la
 cascada de clasificadores. Si una ventana falla en alguna etapa, se
 descarta. Si pasa todas las etapas, se considera como una detección.

 *Post-procesamiento*: Finalmente, se pueden aplicar técnicas como la
 supresión de no máximos para reducir falsos positivos y mejorar la
 precisión.

 *Aplicaciones*: Detección de rostros en imágenes y videos.  Detección
 de peatones u otros objetos en sistemas de vigilancia.  Aplicaciones
 de realidad aumentada.  Es importante mencionar que, aunque los Haar
 Cascades fueron revolucionarios en su momento, han sido superados en
 precisión y velocidad por técnicas más modernas de aprendizaje
 profundo. Sin embargo, siguen siendo utilizados debido a su
 simplicidad y bajo requerimiento de recursos computacionales.

**** Ejemplo de un Haarcascade

https://github.com/opencv/opencv/tree/master/data/haarcascades

https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html

https://docs.opencv.org/2.4/doc/user_guide/ug_traincascade.html

https://amin-ahmadi.com/cascade-trainer-gui/
#+BEGIN_SRC python
import numpy as np
import cv2 as cv
import math 

rostro = cv.CascadeClassifier('haarcascade_frontalface_alt.xml')
cap = cv.VideoCapture(0)
i = 0  
while True:
    ret, frame = cap.read()
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    rostros = rostro.detectMultiScale(gray, 1.3, 5)
    for(x, y, w, h) in rostros:
       frame = cv.rectangle(frame, (x,y), (x+w, y+h), (0, 255, 0), 2)
       frame2 = frame[ y:y+h, x:x+w]
       #frame3 = frame[x+30:x+w-30, y+30:y+h-30]
       frame2 = cv.resize(frame2, (100, 100), interpolation=cv.INTER_AREA)
       
        
       
       cv.imwrite('/home/likcos/recorte/lalo'+str(i)+'.jpg', frame)
       cv.imshow('rostror', frame2)
    cv.imshow('rostros', frame)
    i = i+1
    k = cv.waitKey(1)
    if k == 27:
        break
cap.release()
cv.destroyAllWindows()
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
import cv2 as cv 

rostro = cv.CascadeClassifier('haarcascade_frontalface_alt.xml')
cap = cv.VideoCapture(0)

while True:
    ret, img = cap.read()
    gris = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
    rostros = rostro.detectMultiScale(gris, 1.3, 5)
    for(x,y,w,h) in rostros:
        res = int((w+h)/8)
        img = cv.rectangle(img, (x,y), (x+w, y+h), (234, 23,23), 2)
        img = cv.rectangle(img, (x,int(y+h/2)), (x+w, y+h), (0,255,0),5 )
        img = cv.circle(img, (x + int(w*0.3), y + int(h*0.4)) , 21, (0, 0, 0), 2 )
        img = cv.circle(img, (x + int(w*0.7), y + int(h*0.4)) , 21, (0, 0, 0), 2 )
        img = cv.circle(img, (x + int(w*0.3), y + int(h*0.4)) , 20, (255, 255, 255), -1 )
        img = cv.circle(img, (x + int(w*0.7), y + int(h*0.4)) , 20, (255, 255, 255), -1 )
        img = cv.circle(img, (x + int(w*0.3), y + int(h*0.4)) , 5, (0, 0, 255), -1 )
        img = cv.circle(img, (x + int(w*0.7), y + int(h*0.4)) , 5, (0, 0, 255), -1 )

    cv.imshow('img', img)
    if cv.waitKey(1)== ord('q'):
        break
    
cap.release
cv.destroyAllWindows()
#+END_SRC

#+RESULTS:
: None






**** Eigenfaces 

Un Eigenface (en español cara propia) es el nombre dado a un conjunto
de vectores propios cuando se utiliza en el problema de visión
artificial del reconocimiento de rostros humanos. Sirovich y Kirby
desarrollaron el enfoque de usar caras propias para el reconocimiento
y lo usaron Matthew Turk y Alex Pentland en la clasificación de
caras. Los vectores propios se derivan de la matriz de covarianza de
la distribución de probabilidad sobre el espacio vectorial de alta
dimensión de imágenes de rostros. Las caras propias forman un conjunto
base de todas las imágenes utilizadas para construir la matriz de
covarianza. Esto produce una reducción de la dimensión al permitir que
el conjunto más pequeño de imágenes base represente las imágenes de
entrenamiento originales. La clasificación se puede lograr comparando
cómo se representan las caras por el conjunto base.

 *Generación*
 Se puede generar un conjunto de caras propias mediante la realización
 de un proceso matemático llamado análisis de componentes principales
 (PCA) en un gran conjunto de imágenes que representan diferentes
 rostros humanos. De manera informal, las caras propias pueden
 considerarse un conjunto de "ingredientes faciales estandarizados",
 derivados del análisis estadístico de muchas imágenes de
 rostros. Cualquier rostro humano puede considerarse una combinación
 de estos rostros estándar. Por ejemplo, la cara de uno podría estar
 compuesta por la cara promedio más el 10 % de la cara propia 1, el 55
 % de la cara propia 2 e incluso el −3 % de la cara
 propia 3. Sorprendentemente, no se necesitan muchas caras propias
 combinadas para lograr una aproximación justa de la mayoría de las
 caras. Además, debido a que la cara de una persona no se registra
 mediante una fotografía digital, sino simplemente como una lista de
 valores (un valor para cada cara propia en la base de datos
 utilizada), se ocupa mucho menos espacio para la cara de cada
 persona.

 Las caras propias que se crean aparecerán como áreas claras y oscuras
 que se organizan en un patrón específico. Este patrón es cómo se
 seleccionan las diferentes características de una cara para
 evaluarlas y puntuarlas. Habrá un patrón para evaluar la simetría, si
 hay algún estilo de vello facial, dónde está la línea del cabello o
 una evaluación del tamaño de la nariz o la boca. Otras caras propias
 tienen patrones que son menos fáciles de identificar, y la imagen de
 la cara propia puede parecerse muy poco a una cara.

 La técnica utilizada en la creación de caras propias y su uso para el
 reconocimiento también se utiliza fuera del reconocimiento facial:
 reconocimiento de escritura a mano, lectura de labios, reconocimiento
 de voz, lenguaje de señas /interpretación de gestos con las manos y
 análisis de imágenes médicas. Por lo tanto, algunos no usan el
 término "eigenface", sino que prefieren usar 'eigenimage'.



#+BEGIN_SRC python :results output
import cv2 as cv 
import numpy as np 
import os
dataSet = '/home/likcos/pruebacaras'
faces  = os.listdir(dataSet)
print(faces)

labels = []
facesData = []
label = 0 
for face in faces:
    facePath = dataSet+'/'+face
    for faceName in os.listdir(facePath):
        labels.append(label)
        facesData.append(cv.imread(facePath+'/'+faceName,0))
    label = label + 1
print(np.count_nonzero(np.array(labels)==0)) 

faceRecognizer = cv.face.EigenFaceRecognizer_create()
faceRecognizer.train(facesData, np.array(labels))
faceRecognizer.write('laloEigenface.xml')

#+END_SRC

#+RESULTS:
: ['lalo']
: 169

#+BEGIN_SRC python
import cv2 as cv
import os 

faceRecognizer = cv.face.EigenFaceRecognizer_create()
faceRecognizer.read('laloEigenface.xml')

cap = cv.VideoCapture(0)
rostro = cv.CascadeClassifier('data/haarcascade_frontalface_alt.xml')
while True:
    ret, frame = cap.read()
    if ret == False: break
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    cpGray = gray.copy()
    rostros = rostro.detectMultiScale(gray, 1.3, 3)
    for(x, y, w, h) in rostros:
        frame2 = cpGray[y:y+h, x:x+w]
        frame2 = cv.resize(frame2,  (100,100), interpolation=cv.INTER_CUBIC)
        result = faceRecognizer.predict(frame2)
        #cv.putText(frame, '{}'.format(result), (x,y-20), 1,3.3, (255,255,0), 1, cv.LINE_AA)
        if result[1] > 2800:
            cv.putText(frame,'{}'.format(faces[result[0]]),(x,y-25),2,1.1,(0,255,0),1,cv.LINE_AA)
            cv.rectangle(frame, (x,y),(x+w,y+h),(0,255,0),2)
        else:
            cv.putText(frame,'Desconocido',(x,y-20),2,0.8,(0,0,255),1,cv.LINE_AA)
            cv.rectangle(frame, (x,y),(x+w,y+h),(0,0,255),2)
    cv.imshow('frame', frame)
    k = cv.waitKey(1)
    if k == 27:
        break
cap.release()
cv.destroyAllWindows()

#+END_SRC

#+RESULTS:



**** Fisherfaces


 El algoritmo Fisherfaces es una técnica de reconocimiento facial que
 forma parte del campo del aprendizaje automático y la visión por
 computadora. Este algoritmo es una extensión del método de Análisis de
 Componentes Principales (PCA) y fue diseñado específicamente para
 mejorar la capacidad de reconocimiento en situaciones donde la
 iluminación y las expresiones faciales varían significativamente.

 La idea central detrás de Fisherfaces es reducir la dimensionalidad de
 las imágenes faciales manteniendo al mismo tiempo la capacidad de
 distinguir entre diferentes clases (es decir, diferentes
 personas). Esto se logra mediante el Análisis Discriminante Lineal
 (LDA), que es la base del método Fisherfaces. 

 Preprocesamiento: Las imágenes faciales se normalizan en términos de
 tamaño, orientación e iluminación.

 *Análisis de Componentes Principales (PCA)*: Se realiza PCA para reducir
 la dimensionalidad de los datos. PCA identifica las direcciones en las
 que los datos varían más y proyecta los datos en un espacio de menor
 dimensión preservando estas variaciones principales.

 *Análisis Discriminante Lineal (LDA)*: Después de aplicar PCA, se
 utiliza LDA para encontrar las combinaciones lineales de
 características que mejor separan las diferentes clases (diferentes
 personas). Mientras que PCA busca direcciones que maximizan la
 varianza en los datos, LDA busca maximizar la separación entre las
 diferentes clases.

 *Proyección y Clasificación*: Las imágenes se proyectan en el espacio de
 características obtenido por PCA y LDA. Luego, se utiliza un
 clasificador (como k-NN o máquinas de vectores de soporte) para
 identificar a qué clase (persona) pertenece cada imagen proyectada
 basándose en las características extraídas.

 El algoritmo Fisherfaces es particularmente efectivo en situaciones
 donde las variaciones entre las imágenes de una misma clase (por
 ejemplo, las diferentes expresiones faciales de una persona) son
 menores en comparación con las variaciones entre clases diferentes
 (diferentes personas). Esto lo hace robusto frente a cambios en la
 iluminación y las expresiones faciales, siendo una técnica popular en
 aplicaciones de reconocimiento facial.

#+CAPTION: Script para leer un dataset y generar el entrenamiento con FisherFaces
 #+BEGIN_SRC python ::results
import cv2 as cv 
import numpy as np 
import os

dataSet = '/home/likcos/pruebacaras'
faces  = os.listdir(dataSet)
print(faces)

labels = []
facesData = []
label = 0 
for face in faces:
    facePath = dataSet+'/'+face
    for faceName in os.listdir(facePath):
        labels.append(label)
        facesData.append(cv.imread(facePath+'/'+faceName,0))
    label = label + 1
#print(np.count_nonzero(np.array(labels)==0)) 
faceRecognizer = cv.face.FisherFaceRecognizer_create()
faceRecognizer.train(facesData, np.array(labels))
faceRecognizer.write('laloFisherFace.xml')


 #+END_SRC

 #+RESULTS:
 : None

 #+BEGIN_SRC python
import cv2 as cv
import os 

faceRecognizer = cv.face.FisherFaceRecognizer_create()
faceRecognizer.read('laloFisherFace.xml')

cap = cv.VideoCapture(0)
rostro = cv.CascadeClassifier('data/haarcascade_frontalface_alt.xml')
while True:
    ret, frame = cap.read()
    if ret == False: break
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    cpGray = gray.copy()
    rostros = rostro.detectMultiScale(gray, 1.3, 3)
    for(x, y, w, h) in rostros:
        frame2 = cpGray[y:y+h, x:x+w]
        frame2 = cv.resize(frame2,  (100,100), interpolation=cv.INTER_CUBIC)
        result = faceRecognizer.predict(frame2)
        cv.putText(frame, '{}'.format(result), (x,y-20), 1,3.3, (255,255,0), 1, cv.LINE_AA)
        if result[1] < 500:
            cv2.putText(frame,'{}'.format(faces[result[0]]),(x,y-25),2,1.1,(0,255,0),1,cv2.LINE_AA)
            cv2.rectangle(frame, (x,y),(x+w,y+h),(0,255,0),2)
        else:
            cv2.putText(frame,'Desconocido',(x,y-20),2,0.8,(0,0,255),1,cv2.LINE_AA)
            cv2.rectangle(frame, (x,y),(x+w,y+h),(0,0,255),2)
    cv.imshow('frame', frame)
    k = cv.waitKey(1)
    if k == 27:
        break
cap.release()
cv.destroyAllWindows()



 #+END_SRC

 #+RESULTS:



**** LBPH
El LBPH es un enfoque simple y efectivo para el reconocimiento
facial. A diferencia de otros métodos que operan en todo el rostro, el
LBPH trabaja examinando características locales. Su popularidad se
debe a su simplicidad, velocidad y buen rendimiento, incluso en
condiciones de iluminación desafiantes. Aquí está cómo funciona:

División de la Imagen en Celdas: La imagen del rostro se divide en
pequeñas regiones o celdas.

*Calculo de Patrones Binarios Locales (LBP):* Para cada píxel en una
región, se compara su intensidad con las de sus vecinos (generalmente
8 vecinos circundantes). Si la intensidad del vecino es mayor o igual
que el píxel central, se asigna un 1, de lo contrario un 0. Esto
genera un número binario de 8 dígitos (o un número decimal después de
la conversión) para cada píxel.

*Histogramas:* Se calcula un histograma de estas etiquetas LBP para cada
celda. Los histogramas cuentan la frecuencia de cada número obtenido
en el paso anterior dentro de la celda.

*Concatenación de Histogramas:* Los histogramas de todas las celdas se
concatenan en un solo vector de características. Este vector describe
las características locales de la imagen de la cara.

*Reconocimiento:* Para reconocer un rostro desconocido, se calcula su
vector de características LBPH y se compara con los vectores de
características de las caras conocidas (generalmente usando una medida
de distancia, como la distancia euclidiana). La imagen desconocida se
identifica como la clase (es decir, la persona) cuyo vector de
características conocido sea más cercano al del rostro desconocido.

El LBPH es eficaz en diversas condiciones y no requiere un
preprocesamiento tan intenso como otros métodos de reconocimiento
facial. Puede manejar variaciones en iluminación y expresión facial
bastante bien. Además, su implementación es relativamente sencilla, lo
que lo hace popular para aplicaciones en tiempo real y sistemas
embebidos.

#+BEGIN_SRC python
import cv2 as cv 
import numpy as np 
import os

dataSet = '/home/likcos/pruebacaras'
faces  = os.listdir(dataSet)
print(faces)

labels = []
facesData = []
label = 0 
for face in faces:
    facePath = dataSet+'/'+face
    for faceName in os.listdir(facePath):
        labels.append(label)
        facesData.append(cv.imread(facePath+'/'+faceName,0))
    label = label + 1
#print(np.count_nonzero(np.array(labels)==0)) 
faceRecognizer = cv.face.LBPHFaceRecognizer_create()
faceRecognizer.train(facesData, np.array(labels))
faceRecognizer.write('laloLBPHFace.xml')

#+END_SRC

#+BEGIN_SRC python
import cv2 as cv
import os 

faceRecognizer = cv.face.LBPHFaceRecognizer_create()
faceRecognizer.read('laloLBPHFace.xml')

cap = cv.VideoCapture(0)
rostro = cv.CascadeClassifier('data/haarcascade_frontalface_alt.xml')
while True:
    ret, frame = cap.read()
    if ret == False: break
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    cpGray = gray.copy()
    rostros = rostro.detectMultiScale(gray, 1.3, 3)
    for(x, y, w, h) in rostros:
        frame2 = cpGray[y:y+h, x:x+w]
        frame2 = cv.resize(frame2,  (100,100), interpolation=cv.INTER_CUBIC)
        result = faceRecognizer.predict(frame2)
        cv.putText(frame, '{}'.format(result), (x,y-20), 1,3.3, (255,255,0), 1, cv.LINE_AA)
        if result[1] < 70:
            cv2.putText(frame,'{}'.format(faces[result[0]]),(x,y-25),2,1.1,(0,255,0),1,cv2.LINE_AA)
            cv2.rectangle(frame, (x,y),(x+w,y+h),(0,255,0),2)
        else:
            cv2.putText(frame,'Desconocido',(x,y-20),2,0.8,(0,0,255),1,cv2.LINE_AA)
            cv2.rectangle(frame, (x,y),(x+w,y+h),(0,0,255),2) 
    cv.imshow('frame', frame)
    k = cv.waitKey(1)
    if k == 27:
        break
cap.release()
cv.destroyAllWindows()


#+END_SRC




*** Template Matching

El **Template Matching** (emparejamiento de plantillas) es una técnica utilizada en procesamiento de imágenes y visión por computadora para buscar y encontrar una sub-imagen o patrón dentro de una imagen más grande. 

**Descripción del algoritmo Template Matching**

1. **Entrada**:
   - Una **imagen de entrada**: Es la imagen en la cual se desea buscar la plantilla.
   - Una **plantilla**: Es una sub-imagen más pequeña que se quiere localizar dentro de la imagen de entrada.

2. **Desplazamiento de la plantilla**: 
   - El algoritmo desplaza la plantilla sobre la imagen de entrada y compara las intensidades de los píxeles de la plantilla con los de la región correspondiente en la imagen.

3. **Cálculo de la similitud**:
   - Existen varias formas de medir la similitud entre la plantilla y la región correspondiente en la imagen:
     - **Correlación Normalizada (NCC)**: La correlación normalizada mide el grado de similitud entre la plantilla y la región de la imagen.
     - **Suma de diferencias cuadráticas (SSD)**: Esta métrica mide la diferencia entre los valores de los píxeles correspondientes en la plantilla y la imagen. Se busca minimizar esta diferencia.
     - **Suma de diferencias absolutas (SAD)**: Similar a la SSD, pero se suma la diferencia absoluta entre los valores de los píxeles.

4. **Localización de la mejor coincidencia**:
   - El algoritmo calcula la similitud o diferencia para cada posible ubicación de la plantilla en la imagen de entrada. La posición con el valor más alto de similitud (o más bajo de diferencia) será la ubicación donde la plantilla se ajusta mejor a la imagen.

5. **Resultado**:
   - El algoritmo devuelve las coordenadas de la región que coincide mejor con la plantilla.

**Formulaciones comunes**

1. **Correlación Normalizada (NCC)**:
\(
 R(x, y) = \frac{\sum_{u,v} \left[ T(u, v) - \bar{T} \right] \left[ I(x+u, y+v) - \bar{I} \right]}{\sqrt{\sum_{u,v} \left[ T(u, v) - \bar{T} \right]^2 \sum_{u,v} \left[ I(x+u, y+v) - \bar{I} \right]^2}}
\)

2. **Suma de diferencias cuadráticas (SSD)**:
\(
R(x, y) = \sum_{u,v} \left[ I(x+u, y+v) - T(u, v) \right]^2
\)

3. **Suma de diferencias absolutas (SAD)**:
\(
R(x, y) = \sum_{u,v} \left| I(x+u, y+v) - T(u, v) \right|
\)

**Ventajas del Template Matching**

- **Simplicidad**: Es un método sencillo y directo de encontrar una sub-imagen dentro de otra.
- **Determinístico**: No requiere entrenamiento ni ajustes de parámetros complejos.
- **Aplicaciones**: Útil en aplicaciones de localización precisa.

**Desventajas del Template Matching**

- **Sensibilidad a la escala y rotación**: No funciona bien si la plantilla y la imagen no están en la misma escala o rotadas.
- **Ruido y variaciones de iluminación**: Se ve afectado por el ruido y las variaciones de iluminación.
- **Costoso computacionalmente**: Es lento si la imagen es grande, ya que requiere hacer muchas comparaciones.

**Mejoras del Template Matching**

- **Uso de pirámides**: Se pueden utilizar pirámides de imágenes para mejorar el rendimiento.
- **Métodos más robustos**: Se pueden utilizar métodos avanzados de coincidencia basados en características locales, como SIFT o SURF.


#+begin_src python
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Cargar la imagen y la plantilla en color
imagen_color = cv2.imread('gen.png')  # Imagen donde se buscará la plantilla
plantilla_color = cv2.imread('wall.png')   # La plantilla que queremos encontrar

# Convertir las imágenes a escala de grises para el template matching
imagen_gris = cv2.cvtColor(imagen_color, cv2.COLOR_BGR2GRAY)
plantilla_gris = cv2.cvtColor(plantilla_color, cv2.COLOR_BGR2GRAY)

# Obtener las dimensiones de la plantilla
altura, ancho = plantilla_gris.shape

# Aplicar el método de Template Matching usando la correlación normalizada (TM_CCOEFF_NORMED)
resultado = cv2.matchTemplate(imagen_gris, plantilla_gris, cv2.TM_CCOEFF_NORMED)

# Encontrar la mejor coincidencia usando minMaxLoc
min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(resultado)

# Coordenadas del rectángulo de coincidencia
top_left = max_loc
bottom_right = (top_left[0] + ancho, top_left[1] + altura)

# Dibujar un rectángulo alrededor de la coincidencia sobre la imagen original en color
cv2.rectangle(imagen_color, top_left, bottom_right, (0, 255, 0), 2)

# Convertir BGR a RGB para mostrar con matplotlib
#imagen_rgb = cv2.cvtColor(imagen_color, cv2.COLOR_BGR2RGB)

# Mostrar la imagen con el rectángulo en color
cv2.imshow('resultado', imagen_color)
cv2.imshow('resultado2', resultado)

cv2.waitKey()
cv2.destroyAllWindows()

#+end_src

#+RESULTS:
: None



* Programación

#+BEGIN_SRC python
import cv2 as cv
import numpy as np 

img = cv.imread('/home/likcos/Imágenes/tr.png')

hsv=cv.cvtColor(img, cv.COLOR_BGR2HSV)
ub = np.array([0, 100, 100])
ua = np.array([10,255,255])
ub1 = np.array([170, 100,100])
ua1 = np.array([180,255,255])


mascara1 = cv.inRange(hsv, ub, ua)
mascara2 = cv.inRange(hsv, ub1, ua1)
mascara= mascara1 + mascara2
resultado = cv.bitwise_and(img, img, mask=mascara)
cv.imshow('resultado', resultado)
cv.imshow('manzana', img)
cv.imshow('mascara', mascara)
cv.imshow('hsv', hsv)
cv.waitKey(0)
cv.destroyAllWindows()





#+END_SRC

#+RESULTS:
: None





#+BEGIN_SRC python
import cv2 as cv
import numpy as np

img = cv.imread("/home/likcos/Imágenes/tuli.png", 1)
hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)

ub=np.array([0, 40, 40])
ua=np.array([10, 255, 255])

ub1=np.array([160, 40, 40])
ua1=np.array([180, 255, 255])

mask1 = cv.inRange(hsv, ub, ua)
mask2 = cv.inRange(hsv, ub1, ua1)

mask = mask1 + mask2

res = cv.bitwise_and(img, img, mask=mask)

cv.imshow('res',res )
cv.imshow('hsv', hsv)
cv.imshow('mask', mask)
cv.imshow('img', img)
cv.waitKey(0)
cv.destroyAllWindows()
#+END_SRC

#+RESULTS:
: None


#+BEGIN_SRC python
import cv2 as cv
import numpy as np

cap = cv.VideoCapture(0)
while True:
    ret, img = cap.read()
    if(ret):
        hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)
        ub=np.array([88, 30, 30])
        ua=np.array([92, 255, 255])
        mask = cv.inRange(hsv, ub, ua)
        res = cv.bitwise_and(img, img, mask=mask)

        cv.imshow('res', res)
        #cv.imshow('gray', gris)
        
   	k =cv.waitKey(1) & 0xFF
	if k == 27 :
	    break
    else:
	break
cap.release()
cv.destroyAllWindows()
        
#+END_SRC

#+RESULTS:
: None




#+BEGIN_SRC python :results output
import cv2 as cv
import numpy as np

img = cv.imread("/home/likcos/Imágenes/tr.png", 1)
print(img.shape[:2])
imgn = np.zeros(img.shape[:2], np.uint8)
b,g,r =cv.split(img)
#img2 = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
#img3 =cv.cvtColor(img, cv.COLOR_BGR2HSV)
imgb = cv.merge([b, imgn, imgn])
imgg = cv.merge([imgn, g, imgn])
imgr = cv.merge([imgn, imgn, r])
grb = cv.merge([imgn, r, b])


cv.imshow('b', b)
cv.imshow('g', g)
cv.imshow('r', r)
cv.imshow('img',img)
#cv.imshow('img2', img2)
cv.imshow('imgb', imgb )
cv.imshow('imgg', imgg )
cv.imshow('imgr', imgr )
cv.imshow('grb', grb )



cv.waitKey()
cv.destroyAllWindows()

#+END_SRC

#+RESULTS:
: (632, 635)



#+BEGIN_SRC python :results output
import cv2
import numpy as np

img = cv2.imread("/home/likcos/Imágenes/tr.png", 1)
imgn = np.zeros(img.shape[:2], np.uint8)
print(img.shape)
b,g,r=cv2.split(img)
imgb=cv2.merge([b, imgn, imgn])
imgg=cv2.merge([imgn, g, imgn])
imgr=cv2.merge([imgn, imgn, r])
imgnn = cv2.merge([g ,r , b])
cv2.imshow('salidab', imgnn)
cv2.imshow('salidag', imgg)
cv2.imshow('salidar', imgr)


#img2 = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
#img3 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
#img4 = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
#cv2.imshow('salida1', b)
#cv2.imshow('salida2', imgb)
#cv2.imshow('salida3', r)
#cv2.imshow('salida4', imgn)

cv2.waitKey(0)
cv2.destroyAllWindows()

#+END_SRC

#+RESULTS:
: (632, 635, 3)


#+BEGIN_SRC python :results output
import cv2 as cv
import numpy as np 
img = cv.imread('tr.png',0)
x,y=img.shape
img2 = np.zeros((x*2,y*2), dtype='uint8')
for i in range(x):
    for j in range(y):
        if(img[i,j]>150):
            img[i, j]=255
        else:
            img[i,j] = 0

print(img.shape)
#cv.imshow('img2', img2)
cv.imshow('img', img)
cv.waitKey(0)
cv.destroyAllWindows()

#+END_SRC

#+BEGIN_SRC python
import cv2 as cv

cap = cv.VideoCapture(0)

while(True):
    ret, img =cap.read()
    if ret == True:
        cv.imshow('img', img)
    	k =cv.waitKey(1) & 0xFF
	if k == 27 :
	    break
    else:
	break

cap.release()
cv.destroyAllWindows()

#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python :results output
import cv2 as cv
import numpy as np 
img = cv.imread('tr.png',0)
print(img.shape)
x,y = img.shape
img2 = np.zeros((x*2,y*2), dtype='uint8')
for i in range(x):
    for j in range(y):
        img2[i*2,j*2]=img[i,j]

cv.imshow('img', img)
cv.imshow('img2', img2)
cv.waitKey(0)
cv.destroyAllWindows()

#+END_SRC

#+RESULTS:
: (632, 635)


#+BEGIN_SRC python
import cv2 as cv 

img = cv.imread('tr.png', 1)
gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
rgb = cv.cvtColor(img, cv.COLOR_BGR2RGB)
hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)

cv.imshow('img', img)
cv.imshow('gray', gray)
cv.imshow('rgb', rgb)
cv.imshow('hsv', hsv)

cv.waitKey(0)
cv.destroyAllWindows()

#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python 
import cv2 as cv
import numpy as np 

cap = cv.VideoCapture(0)

while(True):
    ret,img=cap.read()
    if(ret == True):
        cv.imshow('marco', img)
        x,y = img.shape[:2]
        img2 = np.zeros((x,y), dtype='uint8')
       
        #hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)
        #cv.imshow('hsv', hsv)
        b,g,r = cv.split(img)
        bm = cv.merge([b,img2, img2])
        gm = cv.merge([img2, g, img2])
        rm = cv.merge([img2, img2, r])
        eje = cv.merge([b,r,g])
        cv.imshow('b', bm)
        cv.imshow('g', gm)
        cv.imshow('r', rm)
        cv.imshow('eje', eje)
        k =cv.waitKey(1) & 0xFF
	if k == 27 :
	    break
    else:
        break

cap.release()
cv.destroyAllWindows()
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
import cv2 as cv

img = cv.imread('man1.jpg', 1)
hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)
ubb=(0, 100, 100)
uba=(10, 255, 255)
ubb2=(170, 100, 100)
uba2=(180, 255, 255)

mask1 = cv.inRange(hsv, ubb, uba)
mask2 = cv.inRange(hsv, ubb2, uba2)
mask = mask1 + mask2


res = cv.bitwise_and(img, img, mask=mask)
cv.imshow('mask', mask)
cv.imshow('hsv', hsv)
cv.imshow('res', res)
cv.imshow('img', img)
cv.waitKey(0)
cv.destroyAllWindows()




#+END_SRC

#+RESULTS:
: None

** Segmentación de color en vídeo

#+BEGIN_SRC python 
import cv2 as cv
import numpy as np 

cap = cv.VideoCapture(0)

while(True):
    ret,img=cap.read()
    if(ret == True):
        hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)
        ubb=(35, 40, 40)
        uba=(95, 255, 255)

        mask = cv.inRange(hsv, ubb, uba)
        res = cv.bitwise_or(img, img, mask=mask)

        cv.imshow('img', img)
        cv.imshow('res', res)
        
        k =cv.waitKey(1) & 0xFF
	if k == 27 :
	    break
    else:
        break

cap.release()
cv.destroyAllWindows()
#+END_SRC

#+RESULTS:
: None

** Seguimiento Por color 

#+BEGIN_SRC python 
import cv2
import numpy as np

# Iniciar la captura de video desde la cámara
cap = cv2.VideoCapture(0)
# Definir el rango de color que quieres rastrear en el espacio de color HSV (en este caso, azul)
lower_blue = np.array([100, 150, 0])
upper_blue = np.array([140, 255, 255])
img2=None
i=0 
while True:
    # Capturar frame por frame
    ret, frame = cap.read()
    if not ret:
        break
    
    # Convertir el frame de BGR a HSV
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    
    # Crear una máscara que detecte solo el color azul
    mask = cv2.inRange(hsv, lower_blue, upper_blue)
    
    # Filtrar la máscara con operaciones morfológicas
    mask = cv2.erode(mask, None, iterations=2)
    mask = cv2.dilate(mask, None, iterations=2)
    
    # Encontrar contornos en la máscara
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    # Si se encuentra al menos un contorno, seguir el objeto
    if contours:
        # Tomar el contorno más grande
        largest_contour = max(contours, key=cv2.contourArea)
        
        # Encontrar el centro del contorno usando un círculo mínimo que lo rodee
        ((x, y), radius) = cv2.minEnclosingCircle(largest_contour)
        
        # Dibujar el círculo y el centro en el frame original si el radio es mayor que un umbral
        if radius > 10:
            i=i+1
            #cv2.circle(frame, (int(x), int(y)), int(radius), (0, 255, 255), 2)
            #cv2.circle(frame, (int(x), int(y)), 5, (0, 255, 255), -1)
            #cv2.rectangle(frame, (int(x-radius), int(y-radius)), (int(x+radius), int(y+radius)), (0, 0, 255), 3)
            img2 = frame[int(y-radius):int(y+radius), int(x-radius):int(x+radius)]
            cv2.imwrite('/home/likcos/recorte/recorte'+str(i)+'.jpg',  img2)
            
            cv2.imshow('img2', img2)
    # Mostrar el frame
    cv2.imshow('Frame', frame)
    #cv2.imshow('img2', img2)
    #cv2.imshow('Mask', mask)

    # Salir si se presiona la tecla 'q'
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Liberar la captura y cerrar todas las ventanas
cap.release()
cv2.destroyAllWindows()

#+END_SRC

#+RESULTS:
: None




* Tutorial Básico de Pygame

Pygame es una biblioteca de Python que facilita la creación de
videojuegos y aplicaciones gráficas interactivas. Este tutorial te
guiará a través de los conceptos básicos de Pygame, desde la
instalación hasta la creación de una ventana básica y la interacción
con el teclado.

- [[https://www.pygame.org/docs/][Documentación Oficial de Pygame]]


** Instalación de Pygame
Para instalar Pygame, puedes usar `pip`, el gestor de paquetes de Python. Abre tu terminal y ejecuta el siguiente comando:


#+begin_src sh
pip install pygame
#+end_src

** Creación de una ventana básica

El primer paso para cualquier aplicación en Pygame es crear una ventana. Este código inicializa Pygame y abre una ventana de 800x600 píxeles.

#+begin_src python
import pygame
import sys

# Inicialización de Pygame
pygame.init()

# Tamaño de la ventana
ANCHO, ALTO = 800, 600
ventana = pygame.display.set_mode((ANCHO, ALTO))
pygame.display.set_caption("Ventana Básica en Pygame")

# Ciclo principal
while True:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            pygame.quit()
            sys.exit()

    # Rellenar la ventana con un color (blanco)
    ventana.fill((255, 255, 255))

    # Actualizar la ventana
    pygame.display.flip()
#+end_src

** Manejo de eventos

Los eventos en Pygame incluyen entradas del teclado, del ratón, y el cierre de la ventana. A continuación te muestro cómo capturar eventos de teclado.

#+begin_src python
import pygame
import sys

# Inicialización de Pygame
pygame.init()

# Tamaño de la ventana
ANCHO, ALTO = 800, 600
ventana = pygame.display.set_mode((ANCHO, ALTO))
pygame.display.set_caption("Manejo de Eventos en Pygame")

# Colores
BLANCO = (255, 255, 255)
ROJO = (255, 0, 0)

# Posición inicial del cuadrado
x, y = 100, 100
velocidad = 5

# Ciclo principal
while True:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            pygame.quit()
            sys.exit()

    # Movimiento del cuadrado con teclas de flechas
    keys = pygame.key.get_pressed()
    if keys[pygame.K_LEFT]:
        x -= velocidad
    if keys[pygame.K_RIGHT]:
        x += velocidad
    if keys[pygame.K_UP]:
        y -= velocidad
    if keys[pygame.K_DOWN]:
        y += velocidad

    # Dibujar el cuadrado
    ventana.fill(BLANCO)
    pygame.draw.rect(ventana, ROJO, (x, y, 50, 50))

    # Actualizar la pantalla
    pygame.display.flip()
#+end_src

** Dibujar formas y texto

Pygame facilita el dibujo de formas básicas, como rectángulos, círculos, y también permite renderizar texto.

#+begin_src python
import pygame
import sys

# Inicialización de Pygame
pygame.init()

# Tamaño de la ventana
ANCHO, ALTO = 800, 600
ventana = pygame.display.set_mode((ANCHO, ALTO))
pygame.display.set_caption("Dibujo de Formas y Texto en Pygame")

# Colores
BLANCO = (255, 255, 255)
VERDE = (0, 255, 0)
AZUL = (0, 0, 255)

# Fuente de texto
fuente = pygame.font.SysFont(None, 55)

# Ciclo principal
while True:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            pygame.quit()
            sys.exit()

    # Rellenar la ventana
    ventana.fill(BLANCO)

    # Dibujar un rectángulo
    pygame.draw.rect(ventana, VERDE, (150, 150, 200, 100))

    # Dibujar un círculo
    pygame.draw.circle(ventana, AZUL, (400, 300), 75)

    # Renderizar texto
    texto = fuente.render("Hola, Pygame!", True, AZUL)
    ventana.blit(texto, (250, 500))

    # Actualizar la ventana
    pygame.display.flip()
#+end_src

** Cargar y mostrar imágenes

Pygame permite cargar imágenes desde archivos externos. Aquí te muestro cómo hacerlo:

#+begin_src python
import pygame
import sys

# Inicialización de Pygame
pygame.init()

# Tamaño de la ventana
ANCHO, ALTO = 800, 600
ventana = pygame.display.set_mode((ANCHO, ALTO))
pygame.display.set_caption("Mostrar Imágenes en Pygame")

# Cargar la imagen
imagen = pygame.image.load('ruta_a_tu_imagen.png')

# Ciclo principal
while True:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            pygame.quit()
            sys.exit()

    # Dibujar la imagen en la ventana
    ventana.fill((255, 255, 255))
    ventana.blit(imagen, (100, 100))

    # Actualizar la ventana
    pygame.display.flip()
#+end_src

* Web scraping

#+BEGIN_SRC python :results output :tangle code/snscrap.py
import snscrape.modules.twitter as sntwitter

query = "ley fula de tal"
tetes = []

for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):
    if i > 10:  # Limitar a los primeros 10 tweets
        break
    tweets.append((tweet.user.username, tweet.content))

for user, content in tweets:
    print(f"Usuario: {user}")
    print(f"Texto: {content}\n")
#+END_SRC

#+RESULTS:



* Preguntas

** Preguntas para responder con el estudio y análisis a través de algoritmos de inteligencia artificial. 

*** Preguntas Ley al poder Judicial
- ¿El Diagnostico de la ley al poder judicial es conocido y qué estudios expertos tuvieron en cuenta?
- ¿Por qué la reforma no incluyó a las fiscalías a las defensoría y
  sólo se limitó al poder judicial ?
- ¿Qué medidas concretas se van a implementar para evitar la captación
  del crimen organizado y la violencia en el contexto electoral?
- ¿Cómo garantizar que juristas probos y honestos se animen a competir
  públicamente frente a los riesgos de la violencia y que la campaña
  para ser incluidos en las listas no implique negociaciones indebidas?
- ¿Cómo se conforman estos comités de postulación?
- ¿Cómo asegurar la carrera judicial?
- ¿Cómo compatibilizar la incorporación de medidas para preservar la
  identidad de los jueces conocidos en el sistema interamericano como
  "jueces sin rostro" con los estándares interamericanos ?
- ¿Cómo impactará el enorme costo económico que tendrá la
  implementación de esta reforma con la promoción y ?

*** Ley Organismos Autónomos
- ¿Es constitucional esta ley, considerando que algunos organismos
  autónomos están establecidos en la Constitución?
- ¿Cómo afectaría la eliminación de estos organismos a la
  transparencia y rendición de cuentas en el gobierno?
- ¿Qué funciones críticas podrían perder independencia y control al
  pasar al poder ejecutivo o a otras instituciones?
- ¿Existen alternativas para mejorar la eficiencia de los organismos
  autónomos sin eliminarlos?
- ¿Qué sectores de la sociedad civil y grupos de interés se verían
  afectados por la desaparición de estos organismos?




* Introducción a los  Embeddings
Los *embeddings* son representaciones vectoriales densas y de baja
dimensión de elementos de un espacio de características, diseñadas
para capturar relaciones semánticas o estructurales entre dichos
elementos. Este concepto es ampliamente utilizado en el procesamiento
del lenguaje natural (NLP), aprendizaje automático y redes neuronales.

** Concepto de Embedding
Un *embedding* toma objetos de un espacio discreto (como palabras,
frases, imágenes, o nodos de un grafo) y los transforma en un espacio
continuo, típicamente un espacio vectorial de dimensión reducida.

Por ejemplo, en NLP, palabras como "gato" y "perro" pueden tener
embeddings que las representan como vectores cercanos en un espacio de
características, indicando su similitud semántica.

*** Propiedades de los Embeddings
- Representación densa: Cada elemento está representado por un vector
  con valores en un espacio continuo.
- Capacidad semántica: Capturan similitudes y relaciones jerárquicas.
- Reducción dimensional: Transforman datos de alta dimensionalidad en
  un espacio más manejable.

** Ejemplos de Uso
*** Procesamiento del Lenguaje Natural (NLP)
En NLP, los embeddings como Word2Vec, GloVe o FastText convierten
palabras en vectores. Estos vectores permiten realizar tareas como:
- Análisis de sentimientos.
- Traducción automática.
- Recuperación de información.

*** Recomendación de Contenido
Los embeddings son fundamentales para sistemas de
recomendación. Representan usuarios y elementos (como películas,
productos, etc.) en un espacio vectorial, donde la proximidad indica
relevancia.

** Visualización y Reducción Dimensional
Algoritmos como t-SNE o UMAP permiten visualizar datos de alta
dimensión reducidos a espacios de 2D o 3D mediante embeddings.

** Métodos para Generar Embeddings
*** Redes Neuronales
Redes como autoencoders o modelos preentrenados (BERT, GPT) generan
embeddings sofisticados al capturar relaciones complejas en los datos.

*** Factorización Matricial
Técnicas como Singular Value Decomposition (SVD) y métodos
colaborativos generan embeddings basados en relaciones entre filas y
columnas.

*** Modelos Gráficos
Graph Embeddings como Node2Vec o DeepWalk transforman nodos de un
grafo en vectores preservando relaciones topológicas.

** Implementación Básica
Aquí un ejemplo en Python utilizando =sklearn= para generar embeddings
básicos:

#+BEGIN_SRC python
from sklearn.manifold import TSNE
import numpy as np

# Datos de ejemplo (3D)
data = np.array([[1, 2, 3], [2, 3, 4], [5, 6, 7]])

# Generar un embedding 2D
tsne = TSNE(n_components=2)
embedding = tsne.fit_transform(data)

print("Embeddings 2D:", embedding)
#+END_SRC

#+RESULTS:


* ¿Qué es Ollama?
**Ollama** es una herramienta diseñada para integrar y facilitar el uso
de modelos de inteligencia artificial centrados en el procesamiento
del lenguaje natural (NLP). Su objetivo principal es proporcionar un
entorno eficiente para la generación y manejo de texto, ofreciendo
capacidades avanzadas para tareas como redacción automática, análisis
semántico y generación de respuestas contextualmente relevantes.

** Características Principales
1. **Modelos Preentrenados**: Ollama emplea modelos avanzados de
   lenguaje que pueden adaptarse a diversos contextos y tareas
   específicas, aprovechando arquitecturas modernas como Transformers.

2. **Integración Sencilla**: La herramienta permite integrar modelos en
   flujos de trabajo existentes mediante APIs o interfaces compatibles
   con múltiples lenguajes de programación.

3. **Personalización**: Los usuarios pueden ajustar los modelos para
   cumplir con requisitos específicos, adaptando el comportamiento del
   sistema a necesidades únicas.

4. **Procesamiento en Tiempo Real**: Ollama es capaz de generar
   respuestas o textos en tiempo real, lo que lo hace adecuado para
   aplicaciones como chatbots, asistentes virtuales y análisis
   dinámico de contenido.

** Usos Comunes de Ollama
*** Asistentes Virtuales
Ollama se utiliza para crear asistentes virtuales que entienden y responden preguntas humanas de forma natural y precisa.

*** Análisis de Texto
Las capacidades de NLP de Ollama permiten realizar análisis como:
   - Extracción de información clave.
   - Resúmenes automáticos.
   - Clasificación de documentos.

*** Automatización de Redacción
Es ideal para la generación de contenido en masa, como publicaciones en blogs, correos electrónicos personalizados y otros textos creativos.

** Ejemplo de Uso
Un caso práctico de integración de Ollama podría ser mediante una API para generar respuestas automáticas en un chatbot:

#+BEGIN_SRC python
import ollama

# Configuración de cliente
client = ollama.Client(api_key="TU_API_KEY")

# Enviar consulta
response = client.query("¿Cuáles son las ventajas del aprendizaje supervisado?")
print(response)
#+END_SRC

** Conclusión
Ollama es una herramienta poderosa en el ámbito de la inteligencia artificial, diseñada para optimizar el procesamiento y la generación de texto en diversas aplicaciones. Su facilidad de integración y personalización la convierten en una opción ideal para desarrolladores y empresas que buscan soluciones basadas en NLP.





* Spacy

#+BEGIN_SRC python
import spacy

# Cargar modelo de lenguaje en español
nlp = spacy.load("es_core_news_md")

# Texto de ejemplo (puede ser extraído de la Constitución)
texto = """
El Instituto Nacional Electoral (INE) es un organismo público autónomo encargado de organizar elecciones en México. 
Su autonomía está garantizada por el artículo 41 de la Constitución.
"""

# Procesar el texto
doc = nlp(texto)

# Extraer entidades relevantes
for ent in doc.ents:
    print(f"Entidad: {ent.text}, Tipo: {ent.label_}")

# Buscar términos clave
for token in doc:
    if token.text.lower() in ["autonomía", "independencia", "organismo"]:
        print(f"Término clave encontrado: {token.text}, Contexto: {token.sent}")

#+END_SRC

#+RESULTS:


* Proyectos IA Final 

** Actividad 1
*** Implementación del Algoritmo A*
Desarrollar el algoritmo A* utilizando el cascarón proporcionado en el apartado de [[https://ealcaraz85.github.io/IA.io/#org0d76d38][pygames]]. 

**** Requisitos
- La solución debe ser óptima.
- Indicar la lista cerrada al final del proceso.
  
** Actividad 2
*** Solución basada en Árboles de Decisión y Redes Neuronales Multicapa
A partir de la adaptación del juego de Phaser a Python:

**** Tareas
1. Implementar la solución utilizando árboles de decisión.
2. Implementar la solución utilizando redes neuronales multicapa.

**** Objetivo
El jugador debe esquivar una pelota saltando.

**** Ejemplo de Redes Neuronales Multicapa en Python
#+BEGIN_SRC python
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split

# Generar datos artificiales
np.random.seed(0)
X = np.random.rand(1000, 2)  # 1000 puntos con 2 características cada uno
y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Etiqueta 1 si la suma de las características > 1, de lo contrario 0

# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Crear el modelo de red neuronal multicapa
model = Sequential([
    Dense(4, input_dim=2, activation='relu'),  # Capa oculta con 4 neuronas y activación ReLU
    Dense(1, activation='sigmoid')            # Capa de salida con 1 neurona y activación sigmoide
])

# Compilar el modelo
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Entrenar el modelo
model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)

# Evaluar el modelo en el conjunto de prueba
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"\nPrecisión en el conjunto de prueba: {accuracy:.2f}")

# Probar con un nuevo dato
nuevo_dato = np.array([[0.8, 0.3]])  # Ejemplo con características específicas
prediccion = model.predict(nuevo_dato)
print(f"Predicción para {nuevo_dato}: {prediccion[0][0]:.2f}")
#+END_SRC

**** Descripción del ejemplo
1. **Datos artificiales**:
   - Se generan 1000 puntos en 2D, cada uno con dos características.
   - Las etiquetas se asignan como 1 si la suma de las características es mayor que 1, de lo contrario 0.
2. **Modelo de red neuronal**:
   - Capa oculta: Tiene 4 neuronas con activación ReLU.
   - Capa de salida: Tiene 1 neurona con activación sigmoide, lo que da como resultado una probabilidad entre 0 y 1.
3. **Compilación**:
   - Función de pérdida: `binary_crossentropy`, adecuada para clasificación binaria.
   - Métrica: `accuracy` (precisión).
4. **Predicción**:
   - Se pasa un nuevo punto al modelo y se obtiene una probabilidad. Si está cerca de 1, la salida es 1; si está cerca de 0, la salida es 0.

** Actividad 3
*** Identificación de modelos de autos con CNN
Utilizando el archivo `CNNriesgo` que se encuentra en la carpeta de Dropbox:

**** Actividad
Ajustar el dataset para identificar cinco modelos diferentes de autos.

**** Evaluación
- Herramientas utilizadas para la creación del dataset.
- Precisión con la que se detectan los modelos.

** Actividad 4
*** Fundamentación sobre la Reforma al Poder Judicial y Organismos Autónomos
Utilizando algoritmos de inteligencia artificial, responder las siguientes preguntas y fundamentar si estás a favor o en contra de la reforma al poder judicial y a los organismos autónomos.

**** Preguntas para la Ley del Poder Judicial
1. ¿El diagnóstico de la ley al poder judicial es conocido y qué estudios expertos se tuvieron en cuenta?
2. ¿Por qué la reforma no incluyó a las fiscalías y a la defensoría, limitándose solo al poder judicial?
3. ¿Qué medidas concretas se implementarán para evitar la captación del crimen organizado y la violencia en el contexto electoral?
4. ¿Cómo garantizar que juristas probos y honestos se animen a competir públicamente frente a los riesgos de la violencia?
5. ¿Cómo se conforman los comités de postulación?
6. ¿Cómo asegurar la carrera judicial?
7. ¿Cómo compatibilizar la incorporación de medidas para preservar la identidad de los jueces (conocidos en el sistema interamericano como "jueces sin rostro") con los estándares internacionales?
8. ¿Cómo impactará el costo económico de esta reforma en la promoción y el acceso a la justicia?

**** Preguntas para la Ley de Organismos Autónomos
1. ¿Es constitucional esta ley, considerando que algunos organismos autónomos están establecidos en la Constitución?
2. ¿Cómo afectaría la eliminación de estos organismos a la transparencia y rendición de cuentas del gobierno?
3. ¿Qué funciones críticas podrían perder independencia y control al pasar al poder ejecutivo u otras instituciones?
4. ¿Existen alternativas para mejorar la eficiencia de los organismos autónomos sin eliminarlos?
5. ¿Qué sectores de la sociedad civil y grupos de interés se verían afectados por la desaparición de estos organismos?

**** Puntos a evaluar
- Algoritmos utilizados.
- Herramientas generadas para el análisis.
- Datos utilizados para la fundamentación.
- Proceso de análisis.





* Embedding de texto para vectorización en IA

** Introducción
El embedding de texto convierte texto a vectores numéricos para que
las computadoras puedan procesarlo en modelos de aprendizaje
automático. Este proceso es fundamental en tareas como clasificación
de texto, búsqueda semántica, y más.

- Uso de modelos pre-entrenados para generar embeddings.
- Vectorización de frases con `sentence-transformers`.
- Ejemplo práctico de búsqueda semántica.

** Requisitos
1. Python 3.7 o superior.
2. Instalar las siguientes librerías:
   - `transformers`
   - `sentence-transformers`
   - `numpy`

Instala estas librerías ejecutando:
#+begin_src bash
pip install transformers sentence-transformers numpy
#+end_src

** Código básico: Generar embeddings con transformers
Aquí usamos un modelo pre-entrenado para convertir texto a vectores:

#+begin_src python
from transformers zimport AutoTokenizer, AutoModel
import torch llama

# Modelo pre-entrenado
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Texto a procesar
texts = ["Hola mundo", "Aprender IA es interesante"]

# Tokenización
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")

# Generar embeddings
with torch.no_grad():
    outputs = model(**inputs)
    embeddings = outputs.last_hidden_state.mean(dim=1)  # Promedio para representar la frase

print("Embeddings generados:", embeddings)
#+end_src

** Vectorización con sentence-transformers
La librería `sentence-transformers` simplifica la generación de embeddings:

#+begin_src python
from sentence_transformers import SentenceTransformer

# Cargar modelo pre-entrenado
model = SentenceTransformer("all-MiniLM-L6-v2")

# Texto a vectorizar
sentences = ["Hola mundo", "Aprender IA es interesante"]

# Generar embeddings
embeddings = model.encode(sentences)

print("Embeddings generados:")
for i, embedding in enumerate(embeddings):
    print(f"Texto: {sentences[i]}")
    print(f"Vector: {embedding[:5]}...")  # Mostrar solo los primeros valores
#+end_src

** Caso práctico: Búsqueda semántica
Este ejemplo implementa una búsqueda semántica básica:

#+begin_src python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Modelo pre-entrenado
model = SentenceTransformer("all-MiniLM-L6-v2")

# Base de datos de textos
corpus = [
    "La inteligencia artificial es fascinante",
    "El aprendizaje automático mejora cada día",
    "Python es un lenguaje poderoso",
]

# Consultas
query = "¿Qué es interesante sobre IA?"

# Generar embeddings
corpus_embeddings = model.encode(corpus)
query_embedding = model.encode([query])

# Similaridad coseno
similarities = cosine_similarity([query_embedding[0]], corpus_embeddings)[0]

# Resultados ordenados
sorted_idx = np.argsort(similarities)[::-1]
print("Resultados más relevantes:")
for idx in sorted_idx:
    print(f"Texto: {corpus[idx]} - Similaridad: {similarities[idx]:.4f}")
#+end_src


** Recursos adicionales
- Documentación de transformers: [[https://huggingface.co/docs/transformers]]
- Documentación de sentence-transformers: [[https://www.sbert.net/]]




* Bibliografía
bibliography:bibliografia.bib



snscrape facebook-page "https://www.facebook.com/share/v/19XLiKLLL8/" --jsonl > posts.jsonl

snscrape facebook-page "https://www.facebook.com/eduardo.alcc" --jsonl > posts.jsonl

https://www.facebook.com/share/v/19XLiKLLL8/






* Examen

1. ¿De qué manera podría la inteligencia artificial ayudar a analizar las
   ventajas y desventajas de eliminar organismos autónomos en México,
   considerando su impacto en la transparencia y eficiencia
   gubernamental?

2. ¿Qué indicadores podría identificar la IA para medir los posibles
   efectos de la elección popular de jueces, magistrados y ministros en
   la independencia judicial y en la confianza pública hacia el sistema
   de justicia?

3. ¿Cómo podría la IA analizar casos internacionales donde se han
   implementado reformas similares para identificar riesgos y beneficios
   aplicables al contexto mexicano?

4. ¿Qué datos podrían extraerse mediante IA para evaluar si la
   desaparición de organismos autónomos afecta el acceso a derechos
   fundamentales como la protección de datos personales o la
   transparencia?

5. ¿Cómo puede la IA ayudar a simular o predecir los efectos de estas
   reformas en la percepción ciudadana sobre el poder judicial y los
   organismos autónomos en términos de equidad y eficacia?





* Actividad

* Problema: Clasificación de Productos según su Aceptación por Clientes

** Descripción del Problema
Una empresa desea predecir si un producto será aceptado o rechazado
por los clientes basándose en las siguientes características:

1. **Precio relativo**: Precio del producto en comparación con productos
   similares (normalizado entre 0 y 1).
2. **Calidad percibida**: Opinión de los clientes sobre la calidad del
   producto en encuestas (normalizada entre 0 y 1).

**Criterios de Clasificación**:
- Un producto se considera aceptado si cumple:
  - El precio relativo es menor o igual a 0.6.
  - La calidad percibida es mayor o igual a 0.7.
- En caso contrario, será rechazado.

**Entradas del Perceptrón**:
- `x1`: Precio relativo (0-1).
- `x2`: Calidad percibida (0-1).

**Salida del Perceptrón**:
- `1`: Aceptado.
- `0`: Rechazado.

**Conjunto de Datos de Entrenamiento**
| Precio relativo | Calidad percibida | Resultado |
|-----------------+-------------------+-----------|
|             0.5 |               0.8 |         1 |
|             0.6 |               0.9 |         1 |
|             0.7 |               0.6 |         0 |
|             0.4 |               0.5 |         0 |
|             0.3 |               0.9 |         1 |
|             0.8 |               0.4 |         0 |

** Actividades
1. Normalizar los datos de entrada si no están ya en el rango (0-1).
2. Implementar un perceptrón simple que pueda aprender esta clasificación.
3. Encontrar los valores óptimos para los pesos `w1`, `w2` y el sesgo `b` mediante entrenamiento.
4. Graficar la frontera de decisión que separa los productos aceptados de los rechazados.
5. ¿Son los datos linealmente separables?
6. ¿Qué ajustes podrían hacer al modelo para mejorar la predicción?
7. Describir cada una de las partes del modelo implementando
   

* Problema: Clasificación de Clientes según su Perfil Financiero

** Descripción del Problema
Una institución financiera desea clasificar a sus clientes en tres
categorías basándose en su perfil financiero y de comportamiento:

1. **Riesgo Bajo**: Clientes que cumplen con todos los pagos a tiempo y
   tienen ingresos estables.
2. **Riesgo Medio**: Clientes con retrasos esporádicos en los pagos o
   ingresos variables.
3. **Riesgo Alto**: Clientes con historial de impagos o ingresos
   inestables.

**Características de Entrada**:
1. **Historial de pagos**: Porcentaje de pagos realizados a tiempo
   (normalizado entre 0 y 1).
2. **Ingresos mensuales**: Ingresos promedio del cliente (normalizado
   entre 0 y 1).
3. **Relación deuda-ingreso**: Proporción entre la deuda total y los
   ingresos totales (normalizado entre 0 y 1).

**Categorías de Salida**:
- `Riesgo Bajo`: [1, 0, 0]
- `Riesgo Medio`: [0, 1, 0]
- `Riesgo Alto`: [0, 0, 1]

** Conjunto de Datos de Entrenamiento
| Historial de pagos | Ingresos mensuales | Relación deuda-ingreso | Resultado       |
|---------------------|--------------------|-------------------------|-----------------|
| 0.9                | 0.8                | 0.2                     | [1, 0, 0]       |
| 0.7                | 0.6                | 0.5                     | [0, 1, 0]       |
| 0.4                | 0.4                | 0.8                     | [0, 0, 1]       |
| 0.8                | 0.9                | 0.3                     | [1, 0, 0]       |
| 0.5                | 0.7                | 0.6                     | [0, 1, 0]       |
| 0.3                | 0.5                | 0.9                     | [0, 0, 1]       |


** Actividades 
1. Implementar una red neuronal multicapa para clasificar los clientes
   según su riesgo.
2. Encontrar los valores óptimos para los pesos `w1`, `w2` 'wn' y el
   sesgo `b` mediante entrenamiento.
3. Graficar la frontera de decisión que separa los clientes .
4. ¿Son los datos linealmente separables?
5. ¿Qué ajustes podrían hacer al modelo para mejorar la clasificación?
6. Describir cada una de las partes del modelo implementando
